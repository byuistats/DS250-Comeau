<!doctype html><html lang=en-us><head><meta charset=utf-8><title>DS250</title><meta name=generator content="Hugo 0.74.3"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><link rel=stylesheet href=https://byuistats.github.io/DS250-Comeau/plugins/bootstrap/bootstrap.min.css><link rel=stylesheet href=https://byuistats.github.io/DS250-Comeau/plugins/themify-icons/themify-icons.css><link rel=stylesheet href=https://byuistats.github.io/DS250-Comeau/plugins/highlight/hybrid.css><link rel=icon href=https://byuistats.github.io/DS250-Comeau/images/favicon.png type=image/x-icon><link href="https://fonts.googleapis.com/css?family=Roboto:300,400,700&display=swap" rel=stylesheet><style>:root{--primary-color:#02007e;--body-color:#f9f9f9;--text-color:#636363;--text-color-dark:#242738;--white-color:#ffffff;--light-color:#f8f9fa;--font-family:Roboto}</style><link href=https://byuistats.github.io/DS250-Comeau/css/style.min.css rel=stylesheet media=screen><script src=https://byuistats.github.io/DS250-Comeau/plugins/jquery/jquery-1.12.4.js></script><script src=https://byuistats.github.io/DS250-Comeau/plugins/jquery/jquery-ui.js></script><script src=https://byuistats.github.io/DS250-Comeau/plugins/bootstrap/bootstrap.min.js></script><script src=https://byuistats.github.io/DS250-Comeau/plugins/match-height/jquery.matchHeight-min.js></script><script src=https://byuistats.github.io/DS250-Comeau/plugins/highlight/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-132356198-4','auto');ga('send','pageview');}</script></head><body><header class="banner overlay bg-cover" data-background=https://byuistats.github.io/DS250-Comeau/images/banner.png><nav class="navbar navbar-expand-md navbar-dark"><div class=container><a class="navbar-brand px-2" href=/DS250-Comeau>DS250</a>
<button class="navbar-toggler border-0" type=button data-toggle=collapse data-target=#navigation aria-controls=navigation aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse text-center" id=navigation><ul class="navbar-nav ml-auto"><li class=nav-item><a class="nav-link text-dark" href=/DS250-Comeau>Home</a></li><li class=nav-item><a class="nav-link text-dark" href=/DS250-Comeau/projects>Projects</a></li><li class=nav-item><a class="nav-link text-dark" href=/DS250-Comeau/contact>Contact</a></li><li class=nav-item><a class="nav-link text-dark" href=/DS250-Comeau/course-materials>Materials</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle text-dark" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Navigate</a><div class=dropdown-menu><a class=dropdown-item href=/DS250-Comeau/slides>Slides</a>
<a class=dropdown-item href=/DS250-Comeau/course-materials/syllabus/>Syllabus</a>
<a class=dropdown-item href=/DS250-Comeau/faq>FAQ</a></div></li></ul></div></div></nav><div class="container section"><div class=row><div class="col-lg-8 text-center mx-auto"><h1 class="text-white mb-3">DS 250: Data Science Programming</h1><p class="text-white mb-4">Using pandas, Altiar, scikit-learn, and NumPy to program with data</p><div class=position-relative><input id=search class=form-control placeholder="Have a question? Just ask here or enter terms">
<i class="ti-search search-icon"></i><script>$(function(){var projects=[{value:"Day 2: Project 0",label:"<p>Syllabus Questions?  A note about readings\u0026hellip; Tips for asking for help  Slack Google - acquired discernment   Quarto and tradeoffs Project Submissions: HTML  Are we all on the Slack channel? Follow the Slack invitation that is waiting in your student email.\nMethods Checkpoint All the answers will be in the assigned reading or in these slides.\nNotes on Project 0 Installing Packages and Extensions Learn how to install packages by reading the assigned material and by watching the video tutorial on this page.\nThe readings mention a lot of different packages. For Project 0, you need to install at least pandas, altair, numpy, tabulate, and jupyter.\nThe readings will also mention two VS Code extensions you need to install.\nJupyter Notebooks vs. Interactive Python Window Should you decide to use Juypyter Notebooks this semester within VS Code, this is a great guide to get you started.\nOr you can choose to stick with the Python Interactive window like the textbook does.\nUse Your Resources!  Technical documentation Google searches Asking for help on Slack Don\u0026rsquo;t forget the data science lab! (Starts next week.) Question that cannot be answered by the textbook and documentation? Google it. A function you have never seen before? Google it. An error in your code? Google it.  Markdown What is Markdown?  A clean, human readable way to make slick html and pdf documents Used widely among programmers for clean documentation Used widely by Data Scientists to publish results and communicate with stakeholders  Here\u0026rsquo;s a good summary\nQuarto Do all your tinkering in interactive Python or Jupyter notebooks, report finished code, graphs, etc. in Quatro\nQuarto\nNow for some data! Let\u0026rsquo;s get this party started Your turn:  Read in the cars data set Work with you your teams to talk through interesting possibilities for a graph Work on Project 0 Questions and Tasks   Any issues with getting Python installed?     Python VS Code Altair in VS Code     Does everyone have pandas, altiar, numpy, scikit-learn installed?     Video tutorial: how to install packages.  One way to install packages:\npip install pandas altair Maybe a better way to do it: run this in an interactive window.\nimport sys !{sys.executable} -m pip install pandas altair    Does everyone have altair-saver working?     altair_saver Video tutorial     ---------------------------------------------------- Why are we using Altair?    It is built on the VEGA and D3 which are fast and web based.  Grammar of Graphics: Vega-Lite   Technical Paper Website Endorsment      What are we not learning in this course?    Indexing, .loc[] and .iloc[] I may not be experienced enough to understand why I should teach you these. I think they all add complexity to what we are learning in the course and we have elected to avoid it. We will use reset_index() a lot. I think MultiIndex features create complication. I have also elected to use .filter() instead of .loc[] because I like it.\nVirtual Environments Virtual Environments appear to be an important tool as you continue to use Python. We will not be teaching these or supporting these in our course.\nmatplotlib (and any tool leveraging it) It feels old, has a bad api, and isn\u0026rsquo;t declarative.\n   ----------------------------- What can Python Interactive do?    Let\u0026rsquo;s review the power of Python Interactive  # %% in my .py script is much better than Jupyter notebooks (.ipynb).  If we hope to have our code work in a production environment then Jupyter is problematic. Caching and code chunks are problematic https:\/\/medium.com\/@_orcaman\/jupyter-notebook-is-the-cancer-of-ml-engineering-70b98685ee71       Set-up your py script    Setting up your script A good data science .py script will have packages and data loaded at the top. Usually you have a few short commented sentences that descibe the script purpose.\n# %% # import pandas, altair, numpy import pandas as pd import altair as alt import numpy as np # %% # load data # handgrenade data https:\/\/github.com\/byuidatascience\/data4soils\/blob\/master\/data-raw\/cfbp_handgrenade\/cfbp_handgrenade.csv url = \u0026#39;https:\/\/github.com\/byuidatascience\/data4soils\/raw\/master\/data-raw\/cfbp_handgrenade\/cfbp_handgrenade.csv\u0026#39; dat = pd.read_csv(url)    Make a scatter plot with hmx on the x and rdx on the y    To get you started:\nalt.Chart(dat).encode()    Make a spatial plot with hmx colored     Encode the row and column to the axes. Color the hmx points using the \u0026lsquo;goldorange\u0026rsquo; color scheme. Use mark_square() and make the square sizes 500.     -------------------- Create a histogram of hmx     Encode the x-axis as binned. Encode the y-axis as counts. Configure the title to a fontSize of 20. Use properties to place the title.     ----------------------------- How can I get help?     Make sure you read the reading assignments once or twice or five times. Read the guides on the Course Materials page. Post questions in our #cse250_s21_larson slack channel (and try to help others!) Attend the Data Science Lab. Google is your best friend.     -------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/introduction\/day02\/"},{value:"Day 3: Resume Fork and Merge",label:"<p>Remember from last class: pull, add, commit, push. Making edits in another user\u0026rsquo;s repo Breakout Room Activity\nEach student in the breakout room is going to provide feedback on another student\u0026rsquo;s resume. The breakout room should begin with a group discussion about the work you\u0026rsquo;ve each done on your resume and any questions the group has. Then follow the steps below.\n fork the other student\u0026rsquo;s resume repository. Now clone that forked repository to your computer. On your local version of the forked repository, do the following;\nA. Create a new file called edits.md and save it in the main folder or the repository.\nB. Make a few recommendations or notes in the edits.md file that will help the other student improve his or her resume.\nC. add, commit, push your edits.\nD. Go to the forked repo on GitHub and check if the edits.md file shows up online. Now, create a pull request to get your edits into the other student\u0026rsquo;s original repo.  Once you\u0026rsquo;ve given another student feedback, accept any pull requests submitted to your own repo. Continue to edit and improve your resume based on the feedback you received.\nCreating a fork in byuids-resumes Fork your own resume repository into the BYU-I Data Science Resumes group.\nIf you change your resume after you create this fork, you will have to submit a pull request to make sure the final version of your resume shows up in the group.\nThese instructions will help you create a pull request.\nOpen time to finalize your resume </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p6\/d4\/"},{value:"Day 4: Evaluating Our Models, Part 2",label:"<p>Announcements Today:  Continue discussion about evaluating models Try to understand what are models are doing  Evaluating model performance cont Confusion Matrix Why isn\u0026rsquo;t accuracy enough? A confusion matrix is a quick way to see the strengths and weaknesses of your model. A confusion matrix is not a \u0026ldquo;metric\u0026rdquo;. A confusion matrix provides an easy way to calculate multiple metrics such as accuracy, precision, and recall.\nYour Turn With your group, use the links above to find a definition for your assigned metric. Then try using the confusion matrix on the screen to calculate your metric for my model.\n Group 1: Accuracy Group 2: Sensitivity\/Recall Group 3: Precision Group 4: Specificity Group 5: F1 Score Group 6: Balanced Accuracy  Validation metrics   How to choose a good evaluation metric for your Machine learning model Confustion Matrix Example ) Classification Metrics in scikit-learn   #%% # a confusion matrix print(metrics.confusion_matrix(y_test, y_predicted_DT)) #%% # this one might be easier to read print(pd.crosstab(y_test.before1980, y_predicted_DT, rownames=[\u0026#39;True\u0026#39;], colnames=[\u0026#39;Predicted\u0026#39;], margins=True)) #%% # visualize a confusion matrix # requires \u0026#39;.\u0026#39; to be installed metrics.plot_confusion_matrix(classifier_DT, x_test, y_test) \nSome python code # Which metric seems better `accuracy_score()` or `balanced_accuracy_score()`? Why? print(\u0026#34;Accuracy:\u0026#34;, metrics.accuracy_score(y_test, y_predicted_DT)) print(\u0026#34;Balanced Accuracy:\u0026#34;, metrics.balanced_accuracy_score(y_test, y_predicted_DT)) # Confusion matrix print(pd.crosstab(y_test.before1980, y_predicted_DT, rownames=[\u0026#39;True\u0026#39;], colnames=[\u0026#39;Predicted\u0026#39;], margins=True)) metrics.plot_confusion_matrix(classifier_DT, x_test, y_test) # Other metrics print(metrics.classification_report(y_test, y_predicted_DT)) Improving your fit Using different features Let\u0026rsquo;s add Neighborhood to our previous work.\nThe dwellings_neighborhoods_ml data set contains the neighborhood variable \u0026ldquo;nbhd\u0026rdquo; from the original data transformed to use one hot encoding. (Image source.)\nIf we want to use neighborhoods in our classifier, we need to join dwellings_neighborhoods_ml with the other features we\u0026rsquo;ve been using.\n# what we used last class x = dwellings_ml.filter([\u0026#34;livearea\u0026#34;,\u0026#34;basement\u0026#34;,\u0026#34;stories\u0026#34;,\u0026#34;numbaths\u0026#34;]) y = dwellings_ml[[\u0026#34;before1980\u0026#34;]] # adding on the neighborhood data x2 = x.join(dwellings_neighborhoods, how=\u0026#39;left\u0026#39;) \nPicking a different model Gradient Boosting Classifier   A Gentle Introduction to the Gradient Boosting GradientBoostingClassifier documentation Gradient boosting wikipedia page   from sklearn.ensemble import GradientBoostingClassifier boost = GradientBoostingClassifier(random_state=42) boost.fit(x_train, y_train) \nCategory Boosting (catboost)   Video: What is Category Boosting (catboost)? CatBoostClassifier documentation   import catboost as cb model = cb.CatBoostClassifier(iterations=10, depth=6, learning_rate=1, loss_function=\u0026#39;Logloss\u0026#39;, verbose=False) model.fit(x_train, y_train) </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p4\/d4\/"},{value:"Day 4: Exporting JSON",label:"<p>Welcome to class! Spiritual Thought  Silver Rule Matthew 19:16-22 Camel through the eye of the needle =\u0026gt;  \u0026ldquo;with men this is impossible\u0026hellip;\u0026rdquo;    Announcements Question 5 Let\u0026rsquo;s do an example of question 5 using the mtcars data.\nLoad packages and data #%% import pandas as pd import numpy as np import json url_cars = \u0026#34;https:\/\/github.com\/byuidatascience\/data4missing\/raw\/master\/data-raw\/mtcars_missing\/mtcars_missing.json\u0026#34; cars = pd.read_json(url_cars) \nFind all the missing values #%% # method 1: find \u0026#34;official\u0026#34; null values # hp, wt, and vs cars.isnull().sum() #%% # method 2: just look at the data # car, hp, wt, vs, gear cars.head(10) #%% # method 3: look at summaries # the values in \u0026#39;gear\u0026#39; look funny cars.describe() #%% # method 4: count up categories # looks like 4 rows are blank cars.car.value_counts() \nReformat the missing values Remember, you need to reformat your missing values to make them consistent!\nReading the examples in the replace documentation might give you some ideas.\n#%%  # There are a lot of functions # we could use to give the missing values # a consistent format. # `replace()` is one of the easiest # let\u0026#39;s change everything to np.nan cars_new = cars.replace(999, np.nan).replace(\u0026#34;\u0026#34;, np.nan) # or equivalently: cars_new = cars.replace([999, \u0026#34;\u0026#34;], np.nan) # did we get them all? cars_new.isnull().sum() \nSaving JSON files from a pandas dataframe You can save a DataFrame as a JSON file like this:\n#%% # save the new data as a json cars_new.to_json(\u0026#34;my_cars_data.json\u0026#34;) The df.to_json() documentation shows us how to change the way the JSON file is organized. (By row? By column? etc.)\nThis is the format we would like to see in the report:\n[ { \u0026#34;car\u0026#34;: \u0026#34;Mazda RX4\u0026#34;, \u0026#34;mpg\u0026#34;: 21, \u0026#34;cyl\u0026#34;: 6, \u0026#34;disp\u0026#34;: 160, \u0026#34;hp\u0026#34;: 110, \u0026#34;drat\u0026#34;: 3.9, \u0026#34;wt\u0026#34;: 2.62, \u0026#34;qsec\u0026#34;: 16.46, \u0026#34;vs\u0026#34;: 0, \u0026#34;am\u0026#34;: 1, \u0026#34;gear\u0026#34;: 4, \u0026#34;carb\u0026#34;: 4 } ] And here are the various options:\n# %% # Question 5 wants us to \u0026#34;include one record example\u0026#34; # in our md report that \u0026#34;has a missing value\u0026#34; # you can print out a json file like this: json_data = cars_new.to_json() print(json_data) # but that won\u0026#39;t look good in our report. # instead.... #%% # you can do this. # in this format, the json file is # organized\/printed by column json_data = cars_new.to_json() json_object = json.loads(json_data) json_formatted_str = json.dumps(json_object, indent = 4) print(json_formatted_str) # %% # we can change the format of the # json file using \u0026#39;orient\u0026#39; json_data = cars.to_json(orient=\u0026#34;split\u0026#34;) json_object = json.loads(json_data) json_formatted_str = json.dumps(json_object, indent = 4) print(json_formatted_str) # %% # by table json_data = cars.to_json(orient=\u0026#34;table\u0026#34;) json_object = json.loads(json_data) json_formatted_str = json.dumps(json_object, indent = 4) print(json_formatted_str) # %% # by \u0026#34;record\u0026#34; or \u0026#34;row\u0026#34; json_data = cars.to_json(orient=\u0026#34;records\u0026#34;) json_object = json.loads(json_data) json_formatted_str = json.dumps(json_object, indent = 4) print(json_formatted_str) </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p2\/d4\/"},{value:"Day 4: May the ML columns be with you",label:"<p>Welcome to class! Spiritual Thought Announcements Getting the data ready for machine learning. What are machine learning algorithms expecting to see?  We need to handle missing values and categorical features before feeding the data into a machine learning algorithm, because the mathematics underlying most machine learning models assumes that the data is numerical and contains no missing values. To reinforce this requirement, scikit-learn will return an error if you try to train a model using data that contain missing values or non-numeric values when working with models like linear regression and logistic regression. ref\n We have some options when converting categorical features (columns) to numeric.\n If the category contains numeric information (like a range of numbers) we can convert it to a numeric variable by taking the minimum, average, or maximum of the range. Factorization: If the category is an \u0026ldquo;ordinal\u0026rdquo; variable (meaning, there is an order to the categories) we can assign each category to an integer. (For example, good = 1, better = 2, best = 3.) One-hot Encoding or Dummy Variables: If the category is a \u0026ldquo;nominal\u0026rdquo; variable (without an order) then we need to use one-hot encoding (sometimes called \u0026ldquo;dummy variable encoding\u0026quot;). If the category is some version of True\/False or Yes\/No then we can simply convert the values to zeros and ones.  What\u0026rsquo;s our game plan for the Star Wars columns? 1. Break into Groups Strategize \u002b Code \u002b Share  Group 1: How are you going to turn Age, Income and Education into numbers? Group 2: How are you going to encode  Who Shot First Gender Location All the Yes\/No responses   Group 3: How are you going to deal with the character rankings?  2. Combine all the factors into one big X dataframe 3. Define Y as those making \u0026gt; $50k First: Limit the data to only people who answered \u0026ldquo;Yes\u0026rdquo; to the question \u0026ldquo;Have you seen any of the 6 films in the Star Wars franchise?\u0026rdquo;.\nThen: Use the table below as a guide to prepare your data for machine learning.\n   Column Original Format Convert To     age category (ordinal, age ranges) number   income category (ordinal, income ranges) number   education category (ordinal, name of degree) number   shot_first category (nominal) one-hot   gender category (nominal) one-hot   location category (nominal) one-hot   fan_star_wars Yes\/No 0\/1   expanded_universe Yes\/No 0\/1   fan_exapanded Yes\/No 0\/1   fan_star_trek Yes\/No 0\/1   seen_i Yes\/No (name of movie\/NaN) 0\/1   seen_ii Yes\/No (name of movie\/NaN) 0\/1   seen_iii Yes\/No (name of movie\/NaN) 0\/1   seen_iv Yes\/No (name of movie\/NaN) 0\/1   seen_v Yes\/No (name of movie\/NaN) 0\/1   seen_vi Yes\/No (name of movie\/NaN) 0\/1   movie rankings number -   character rankings category (ordinal) one-hot or factorize    What functions can we use to convert the categorical columns to numeric?  Range of numbers: str.split() and astype() Ordinal: str.replace() Ordinal: pd.factorize() (can also be used for True\/False) Nominal: pd.get_dummies()  Using the drop_first = True option in get_dummies()    Question: When and why would we drop the first column when we convert a category using pd.get_dummies()?\nAnswer: Whenever your algorithm needs to calculate a matrix inverse.\n The one-hot encoding creates one binary variable for each category.\nThe problem is that this representation includes redundancy. For example, if we know that [1, 0, 0] represents \u0026ldquo;blue\u0026rdquo; and [0, 1, 0] represents \u0026ldquo;green\u0026rdquo; we don\u0026rsquo;t need another binary variable to represent \u0026ldquo;red\u0026rdquo;, instead we could use 0 values for both \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;green\u0026rdquo; alone, e.g. [0, 0].\nThis is called a dummy variable encoding, and always represents C categories with C-1 binary variables. In addition to being slightly less redundant, a dummy variable representation is required for some models.\nFor example, in the case of a linear regression model (and other regression models that have a bias term), a one hot encoding will case the matrix of input data to become singular, meaning it cannot be inverted and the linear regression coefficients cannot be calculated using linear algebra. For these types of models a dummy variable encoding must be used instead.\n Source\n   \u0022, \u0022\u0022)) .astype(\u0027float\u0027) .age_min) ``` You can combine the different features (columns) together using [pd.concat()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.concat.html). ```python dat_numeric = pd.concat([ (dat.age .str.split(\u0022-\u0022, expand = True) .rename(columns = {0: \u0027age_min\u0027, 1: \u0027age_max\u0027}) .apply(lambda x: x.str.replace(\u0022 \u0022, \u0022\u0022)) .astype(\u0027float\u0027).age_min), (dat.household_income .str.split(\u0022-\u0022, expand = True) .rename(columns = {0: \u0027income_min\u0027, 1: \u0027income_max\u0027}) .apply(lambda x: x.str.replace(\u0022\\$|,|\\\u002b\u0022, \u0022\u0022)) .astype(\u0027float\u0027).income_min), (dat.education .str.replace(\u0027Less than high school degree\u0027, \u00279\u0027) .str.replace(\u0027High school degree\u0027, \u002712\u0027) .str.replace(\u0027Some college or Associate degree\u0027, \u002714\u0027) .str.replace(\u0027Bachelor degree\u0027, \u002716\u0027) .str.replace(\u0027Graduate degree\u0027, \u002720\u0027) .astype(\u0027float\u0027))], axis = 1 ) ``` Use `pd.get_dummies()` or other functions from these slides to finish preparing the columns for machine learning. Below is one example witih `pd.get_dummies()`. What difference does the `drop_first` option make? ```python dat_onehot = pd.get_dummies(dat.filter([\u0027shot_first\u0027])) dat_onehot = pd.get_dummies(dat.filter([\u0027shot_first\u0027]), drop_first = True) ``` When you\u0027re done, you can use `pd.concat()` again to combine all your features. ```python dat_ml = pd.concat([ # all of the movie rankings (already numbers, no conversion needed), # age, income, and education variables # all the \u0022one-hot\u0022 encoded variables # all the 0\/1 encoded variables ], axis = 1).dropna() ``` ----------------------------------------- Predicting income. Grand Question 4 wants us to \u0026ldquo;build a machine learning model that predicts whether a person makes more than $50k\u0026rdquo;.\nWhat is the target we\u0026rsquo;re interested in?    Aka, what is our \u0026ldquo;outcome\u0026rdquo; or \u0026ldquo;response\u0026rdquo; that we want to predict?\ndat_ml.income \u0026gt; 50000    How to format the features (x) and target (y)    Remember not to include the answer (income) in your features!\nx = dat_ml.drop([\u0026#39;income\u0026#39;], axis = 1) The response needs to be saved as a 0\/1 variable (at least, for binary classification algorithms).\ny = (dat_ml.income \u0026gt; 50000) \/ 1    One example of a model    First we need to build and train the model.\nfrom sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_text # split the data (x) and response (y) into training and testing sets x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .33, random_state = 2020) # build and train the model decision_tree = DecisionTreeClassifier(random_state=0, max_depth=5) decision_tree = decision_tree.fit(x_train, y_train) # what does the decision tree look like? r = export_text(decision_tree, feature_names=x_train.columns.to_list()) print(r) Then we can test it to see how well it does.\nfrom sklearn import metrics # make predictions with the test data predict_y = decision_tree.predict(x_test) # how well did our model do? metrics.plot_confusion_matrix(decision_tree, x_test, y_test) print(metrics.accuracy_score(y_test, predict_y))    ----------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p5\/d4\/"},{value:"Day 4: Telling the story of names",label:"<p>Welcome to class! Spiritual Thought Announcements Altair videos  What is Altair? Altair\u0026rsquo;s Visualization Grammar Altair\u0026rsquo;s Data Types  Coding example and Q\u0026amp;A\n--------------------- Altair examples Adding a subtitle     Example 1 Example 2     Adding a text label     Example 1 Example 2 Example 3     Want to use a bar chart? Try changing the color     Example 1     </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p1\/d4\/"},{value:"DS 250 Competency",label:"<p> We need skills not grades! Shifting Attention\n Competency scale You must complete all competency items at the level detailed to achieve the listed grade. You can request half-step adjustments if you fall slightly short or over on some elements.\nYou will need to provide a detailed description in your request and review letter of the items you completed to support your grade request. If a student was in the B range they might write the following;\n I got three fives, one four, and two threes for 29 points on the projects. I met the checkpoint requirements with 4 full mark mid-project checkpoints and 4 methods and calculation checkpoints. I regularly attended data science society. I believe my coding challenge will be above a 3. I request a B\u002b.\n   Leader (A)    Element Requirement Description     Projects 34 Points 5 points per project   Mid-project checkpoints 5 completed Full credit   Methods \u0026amp; Calculations checkpoints 6 completed 100% unlimited attempts   DS Community Complete all \u0026ndash;   Request and review letter submission \u0026ndash;   Coding challenge At least 3 Score is out of 4    See the competency descriptions in the next section. You can request half-step adjustments if you fall slightly short or over on some elements.\n Supporter (B)    Element Requirement Description     Projects 29 Points 5 points per project   Mid-project checkpoints 3 completed Full credit   Methods \u0026amp; Calculations checkpoints 5 completed 100% unlimited attempts   DS Community Complete 2 items \u0026ndash;   Request and review letter submission \u0026ndash;   Coding challenge At least 3 Score is out of 4    See the competency descriptions in the next section. You can request half-step adjustments if you fall slightly short or over on some elements.\n Listener (C)    Element Requirement Description     Projects 24 Points 5 points per project   Mid-project checkpoints 3 completed Full credit   Methods \u0026amp; Calculations checkpoints 3 completed 100% unlimited attempts   DS Community Complete 1 item \u0026ndash;   Request and review letter submission \u0026ndash;   Coding challenge At least 2 Score is out of 4    See the competency descriptions in the next section. You can request half-step adjustments if you fall slightly short or over on some elements.\n Asleep (D)    Element Requirement Description     Projects 14 Points 5 points per project   Mid-project checkpoints 1 completed Full credit   Methods \u0026amp; Calculations checkpoints 2 completed 100% unlimited attempts   DS Community None \u0026ndash;   Request and review letter None \u0026ndash;   Coding challenge None Score is out of 4    See the competency descriptions in the next section. You can request half-step adjustments if you fall slightly short or over on some elements.\n   Competency elements  Projects (Grand questions) Each of the seven projects is worth 5 points and you get one additional submission after the due date. There are 5 two-week projects and two one-week projects to start and end the class.\nGrading Details  1 point: Submission 3 points: submission of a good faith attempt with a statement of work quality. 4 points: High-quality work that addresses each of the Grand Questions and a statement of work quality. 5 points: Addressed reviewer issues and completion of resubmission if needed.   Checkpoints (methods and calculations) These checkpoints are in Canvas and they open when the project starts. They have unlimited attempts and remain open until the end of the semester.\nExamples  Fact-Finding Questions (Calculate descriptive summaries): Fact-finding questions help you with calculations that build into the Grand Questions of the project. These questions have clearly defined answers using Python calculations. You should expect 2-3 problems.   Example: Using the top 10 airports in size, what is the average size? Example: What proportion of flights are delayed at the largest airport?   How the code works questions (Explaining the tools): This part could have direct answer questions or open-ended questions.   Example (direct): What is the recommended function for arranging your data by a variable? What are the outputs after using \u0026lt;FUNCTION\u0026gt;? Example (open): Your client has shown some confusion about NumPy\u0026rsquo;s \u0026lsquo;nan\u0026rsquo; handling in Python. Help them understand by answering the question, \u0026lsquo;How is missing data handled in Pandas?\u0026rsquo;   Checkpoints (Mid-project status) The mid-project checkpoint has a few questions. It opens the first day of the project and closes at 1 am on the 3rd day of class for the project. It has the following questions.\nExamples  Have you checked off more than one grand question from the current project? (Yes\/No) Have you spent at least 2 hours using code to tackle problems related to the case study? (Yes\/No). Have you prepared questions you have about the case study to ask in your next meeting? (Yes\/No).   Data science community To earn credit for the DS Community element you must complete two different tasks from the list below. At the end of the semester, you will be asked to report on which tasks you completed and what you learned from them.\n Attend Data Science Society at least once. Sign up for an email newsletter that will teach you more about data science. Data Science Weekly or Data Elixir are good options. Listen to a podcast episode about data science. Build a Career in Data Science has some excellent episodes. Watch a professional presentation on YouTube about data science. Be prepared to share the link and a summary of the video. Reach out to someone who works in a data-related field and ask them for 15 minutes of their time. Use this time to conduct an \u0026ldquo;informational interview\u0026rdquo; and learn more about their responsibilities and career path. Research and apply to at least 5 data-related jobs or internships.   Finishing the semester Submit a request and review letter that includes what you have learned from this class, the next data science course you plan on taking, and the final grade that you are requesting based on the work you have submitted.\n Coding challenge We will have an in-class coding challenge on the ultimate or penultimate day of class. It would be best if you did not view this challenge like a traditional exam. It will cover the general techniques that we have been practicing throughout the course.\nWe expect to have a few practice challenges throughout the semester. We will score the coding challenge on a four-point scale.\n 1 point: At least you tried. 2 points: You have learned some items from the course, but your work in the coding challenge is deficient. 3 points: Your submission uses proper coding techniques and addresses the objective. 4 points: Exceptional work. Your code can be used as a solution to share with others.     </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/syllabus\/competency\/"},{value:"DS 250 Syllabus",label:"<p> Most people would sooner die than think, and most of them do. -Bertrand Russell-\n Overview This course provides a better understanding of data programming. If you have signed up for this class, you are most likely driven by curiosity and interested in how data decisions are made (sometimes called data intuition). Possibly, you have a more empathetic approach to how the world works and how problems can be solved. Finally, you have an eye for how society reports and uses data to make impactful decisions.1.\nUpon completing this course, you will be able to use data-driven programming in Python to handle, format, and visualize data. We will introduce you to data wrangling techniques, analytical methods, and the grammar of graphics. Specifically, as a successful learner, you will be able to;\n Use functions, data structures, and other programming constructs efficiently to process and find meaning in data. Programmatically load data from various types of data sources, including files, databases, and remote services. Use data manipulation libraries to perform straightforward analysis, produce charts, and prepare data for machine learning algorithms. Use machine learning libraries to discover insights, make predictions, and interpret the success of these algorithms. Use industry-leading tools to collaborate and share your work.  Principles of DS teaching The course follows these principles of teaching Data Science2\n Organize the course around a set of diverse projects Integrate computing into every aspect of the course Teach abstraction, but minimize reliance on mathematical notation Structure course activities to realistically mimic a data scientist\u0026rsquo;s experience Demonstrate the importance of critical thinking\/skepticism through examples  Competency assumptions This course focuses on programming with data to find insights. The prerequisite for this course is an introductory programming course in Python (CSE 110)3. We recommend taking CSE 111 before or during the same semester you take this course - especially if programming is complicated for you. We assume that you do know what the Terminal is and how to execute scripts.\nAn understanding of standard deviation{target=\u0026quot;blank\u0026rdquo;} and variance{target=\u0026quot;blank\u0026rdquo;} will be valuable.\nCourse materials and structure This course focuses on building core data science skills. You will learn to program, but you will also learn how to communicate and collaborate with your peers and mentors.\nCourse communication  How do I talk with my teacher, TA, and other students in this class?\n  We use Slack for most class and one-on-one communication. Don\u0026rsquo;t email or direct message using I-Learn.\nA. Should I paste code snippets in our class Slack channel to get help? Yes.\nB. Should I ask questions about the projects and the readings in our class Slack channel? Yes.\nC. Should I post random quotes or videos in our class Slack channel? No. Use the #random channel. All assignments are submitted in I-Learn. A. Each project submission requires you to submit a short message to the teacher about your work.\nB. We will respond to your message with edits you can make to earn full credit on your resubmit.\nC. Class announcements about the grading of projects are posted in I-Learn.  Online reading materials  Python for Data Science: A port of R for Data Science using the Python packages pandas and Altair. pandas User Guide Altair User Guide Python Data Science Handbook SQL  Preparation In my experience, getting lectured training outside of college is even more expensive than it is in college. A week\u0026rsquo;s worth of training can cost more than a semester of school here at BYUI. Due to this expense, learning how to digest online material gain understanding before going to the expert with questions is a valuable skill to develop. I expect that you have completed the assigned reading material before class begins.\nSpecifications grading Grading is a nasty side effect of mass learning and academia. We are in a class at a university and will have to manage this side effect. However, we don\u0026rsquo;t have to let it control our learning, thinking, or this class. Learning and thinking should motivate each activity.\nAs we team, teacher and student, we have the challenge to become more! We have worked hard to identify the specifications needed for a python user of the pandas and Altair packages. Our goal is to align your grade with the skill specification you have mastered. In other words, the grade you want will determine how much work you will do. We will not score individual tasks in the class on a percentage scale. If your work meets the specified criteria, you will get full credit.\nIn a specifications-grading system, all tasks are evaluated on a high-standards pass\/fail basis using detailed checklists of task requirements and expectations4. You earn your letter grade by earning passing marks on a set of tasks. This system provides various choices and is closer to how learning and work occur in the real world. It will be easy for us to tell if work is complete, done in good faith, and consistent with the requirements.\n  https:\/\/medium.com\/@nikhilbd\/what-makes-a-good-data-scientist-engineer-a8b4d7948a86#.jr80wl98y. I suppose some of you are just taking this class because your degree says you can, and it fits in your schedule. If so, we should chat to make sure this is the right class for you. \u0026#x21a9;\u0026#xfe0e;\n https:\/\/arxiv.org\/ftp\/arxiv\/papers\/1612\/1612.07140.pdf. You will see this pattern in DS 350, DS 460, and Math 488. It will progressively get more realistic. \u0026#x21a9;\u0026#xfe0e;\n We do expect that this is not your first experience with Python and VS Code. If you have done other programming courses, you should be able to succeed in this course. If you have any questions, please ask. \u0026#x21a9;\u0026#xfe0e;\n Making the right checklists can be difficult. Bad checklists could fall in the following categories \u0026ndash; vague and imprecise; too long; hard to use; impractical; too pedantic. Useful checklists are precise, efficient, easy to use and understand. This is the first time this course has been offered, so we will have to work together to ensure the requirements are reasonable. \u0026#x21a9;\u0026#xfe0e;\n   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/syllabus\/"},{value:"Introduction",label:"<p>A competent student should be able to finish the exercises within 60 minutes. You should work through it on your own. This serves as an assessment of your understanding of the assigned readings.\nBefore you start Make sure you have installed VS-code, pandas, and altair on your computer. You can install these package by typing this line in the terminal.\npip install pandas altair\nOR if you have more than one version of python\npip3.9 install pandas altair\npip3.9 indicates the version of python you are installing the packages to.\nPart 1 Get familiar with your tools Programming involves a lot of research. Unlike subjects like Mathematics or History, we are not required to remember every single function and its usage. It is natural for experienced programmers to look for answers on the internet, books, even from other people\u0026rsquo;s code. Programming will be extremely frustrating if we are not allowed to do web searches, so please get familiar with the tools you have and use them often.\nOffical Documentation This should be your first resort for understanding any code\/function. Scanning the documentation of a function will allow you to get an overview of its usage.\nHere is a link to the documentation of the assign() function:\n(https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.assign.html)\nExample of assign() (as shown in the documentation)\nimport pandas as pd df1 = pd.DataFrame({\u0026#39;temp_c\u0026#39;: [17.0, 25.0]}, index=[\u0026#39;Portland\u0026#39;, \u0026#39;Berkeley\u0026#39;]) df2 = df1.assign(temp_f=df1.temp_c * 9 \/ 5 \u002b 32) Exercise 1: After reading the documentation for assign(), write a short paragraph to explain assign() as if you were talking to someone with zero programming experience (use the example above to help you explain assign()).\n What is the difference between df1 and df2? How was df2 derived from df1?)  Online textbook It pains us to see students would rather be stuck at problems for hours yet they refuse to use the textbook. This is another very useful resource since this is designed for this class. link to the textbook: (https:\/\/byuidatascience.github.io\/python4ds\/)\nExercise 2: Locate the section where the textbook talks about query() and answer these questions.\n What function in R\u0026rsquo;s dplyr is equivalent or comparable to query() in pandas (You should include the section number in your answer)? What is the easiest mistake for python beginner to make that was shown in the text about query() (You should include the section number in your answer)?  The internet Google is a programmer\u0026rsquo;s friend. Get used to googling thing, in fact, you want to be an expert in googling\n Question that cannot be answered by the textbook and documentation? Google it. A function you have never seen before? Google it. An error in your code? Google it.  Exercise 3: Provide at least 2 extra resources you could find about the pandas function drop() on the internet.\nTutor, TA (Through slack, zoom, or in-person) We want to help you with your work; we want to answer your questions; but most importantly, we want to help you succeed in this class. That will require you to put in the necessary time in understanding the readings, coding and debugging. When you ask us a question, we expect that you have read the documentation, searched the textbook, and done your own research. Then we can be most helpful and can provide insights on top of your understanding.\nExamples of bad questions  How does drop() work? We will ask you to read the documentation for drop(). How do you make a table in a markdown file? We will refer you to the textbook. I don\u0026rsquo;t want these columns in my data, how can I drop them? We will ask you if you have found any things on the internet.  Examples of good questions  I am still confused about the syntax of drop(). After reading the documentation, this is my understanding of the function\u0026hellip; . What am I missing? I tried making a table in markdown (show code), it is still not giving me what I want, how can I fix this? I am trying to drop these columns in my dataframe, I think drop() is what I am looking for. Am I in the right direction? If not, what keywords should I be googling?  Exercise 4:\nUsing the code and tools mentioned above, finish question 4 and 5 under 3.2.4 in the textbook.(use the data in mpg for your plot):\n# library import import pandas as pd import altair as alt # data import url = \u0026quot;https:\/\/github.com\/byuidatascience\/data4python4ds\/raw\/master\/data-raw\/mpg\/mpg.csv\u0026quot; mpg = pd.read_csv(url)   Question 4: Make a scatterplot of hwy vs cyl.\n  Question 5: What happens if you make a scatterplot of class vs drv? Why is the plot not useful?\n  After you have completed this skill builder with your team (or on your own) then compare your work to our script    See the script.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/introduction\/"},{value:"Project 0: Introduction",label:"<p>Background We will complete six projects during the semester that each take about four days of class. On average, a student will spend 2 hours outside of class per hour in class to complete the assigned readings, submit any Canvas items, and complete the project (for a total of 8 hours per project). The instruction for each project will be structured into sections as written on this page.\nThis first Background section provides context for the project. Make sure you read the background carefully to see the big picture needs and purpose of the project.\n Python and VS Code are tools commonly used in the field of data science. During our first two days of class we will get VS Code prepped for data science programming. Completing Project 0 will set you pu for success the rest of the semester.\nData Every data science project should start with data, and our class projects are no different. Each project will have \u0026lsquo;Download\u0026rsquo; and \u0026lsquo;Information\u0026rsquo; links like the ones below.\n Download: mpg data\nInformation: Data description\nReadings The Readings section will contain links to reading assignments that are required for each project, as well as optional references. Remember that you are reading this material to build skills. Take the time to comprehend the readings and the skills contained within.\nWe recommend reading through the assigned material once for a general understanding before the first day of each project. You will reread and reference the material multiple times as you complete the project.\n The readings listed below are required for the first two days of class.\n Python for Data Science (P4DS): Introduction P4DS: Data Visualization Section 3.1 \u0026amp; 3.2 Only Saving Altair charts Quarto for DS  Optional References  VS Code user interface Reading Technical Documentation  Questions and Tasks: This section lists the questions and tasks that need to be completed for the project. Your work on the project must be compiled into a rport and submitted in Canvas by the weekend following the last day of material for the project.\n  Finish the readings and be prepared with any questions to get your environment working smoothly (class for on-campus and Slack for online) In VS Code, write a python script to create the example Altair chart from section 3.2.2 of the textbook (part of the assigned readings). Note that you have to type chart to see the Altair chart after you create it. Your final report should also include the markdown table created from the following (assuming you have mpg from question 2).  print(mpg .head(5) .filter([\u0026#34;manufacturer\u0026#34;, \u0026#34;model\u0026#34;,\u0026#34;year\u0026#34;, \u0026#34;hwy\u0026#34;]) .to_markdown(index=False)) Deliverables: Deliverables are “the quantifiable goods or services that must be provided upon the completion of a project”. In this class the deliverable for each project is a HTML report created using Quarto. This final section will be the same for each project.\n Use this template to submit your Client Report. The template has three sections (for additional details please see the instructional template):\n A short summary that highlights key that describes the results describing insights from metrics of the project and the tools you used (Think “elevator pitch”). Answers to the grand questions. Each answer should include a written description of your results, code snippets, charts, and tables.  This is a simple note.\n This is a simple tip.\n This is a simple info.\n --  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/introduction\/"},{value:"Pull and Merge Forks on GitHub",label:"<p>Create Pull Request   Go the the forked repository in byuids-resumes and click Pull request.    This will bring you to the the following page where you need to click switching the base.    Now you can Create pull request.    Here you can type a note and then actually Create pull request.    Now you need to View pull request.   Merge Request If you have admin access of the forked repository where you are doing the pull request, you can finish the next two steps.\n Click the Merge pull request button.    Now confirm the merge.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/git_github_ds\/pull_merge\/"},{value:"Week 12-13: Project 6 - Github",label:"<p> GitHub is the communication tool for Data Scientists and developers. As students, you will want to curate your creative work on GitHub using Git. GitHub is the place to share your original work, not your homework assignments. Many people store their personal websites, blogs, and project websites on GitHub. Our textbook and course are hosted on GitHub, and you can see J. Hathaway\u0026rsquo;s or Ryan Hafen\u0026rsquo;s personal Data Science websites that are hosted on GitHub as well. You will be making your public resume that will be hosted on GitHub for this project.\nIn the process of this project, we will be learning the process of Git and the tools of GitHub. We will use the Git process to have others in our class to edit our resumes. Take the process seriously (pick a suitable username and write a good resume), and you will have the beginning of your social presence in the DS\/CS space.\n Completed Readings: GitHub, a programmer\u0026rsquo;s social media, Join GitHub, Repository Templates, Using Version Control in VS Code, Working with GitHub in VS Code, Git in Visual Studio Code video, New to Git and GitHub? This Essential Beginners Guide is for you, Git vs. GitHub: What is the difference between them?\n Markdown Resume (mdresume) Repository and BYUI Data Science Resumes\n Grand Questions  Join the BYUI Data Science Resumes GitHub organization and use the template repository to make a resume repository under your repositories. A good name might be LASTNAME-Resume. Clone your repository to your computer and build a first draft of your resume. Push your results to GitHub and have another student fork your repository to make edits. Accept the proposed changes from the student review and finish your final version. Make sure your resume is forked by BYU-I Data Science Resumes  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p6\/"},{value:"Day 1: Welcome",label:"<p>Welcome to DS 250!  Teacher: Luc Comeau TA: ???  Announcements  Computing Lab 4:30PM - 6:30PM all weekdays except Wednesday. Saturday from 10AM-12PM  Slack channel #tutoring_lab   Data Science Society - Wednesday\u0026rsquo;s at 6PM  What is a Data Scientist? Class Structure  Problem Solving Improved coding skills Effective written\/visual communication Collaboration Timeliness and communication with \u0026ldquo;the boss\u0026rdquo;  Syllabus\nGot Slack? Are we all on the Slack channel? Follow the Slack invitation that is waiting in your student email.\nWho are you?  Introduce yourself and learn the names\/majors\/origin story of your group members. Make a plan to get help this semester. How will you contact each other? Some ideas: Slack, I-Learn, emails, group texts, etc. If you were independently wealthy, what would you be doing right now? Would you change majors? Highlights of 2022  Problem Solving This is not a \u0026ldquo;see and repeat\u0026rdquo; programming class!\nHow would you go about fixing my motorcycle? Learn how to ask for help (1 hr rule)  Getting started on Project 0 Setting up your Programming Snvironment  Download Visual Studio Code Download Python v (3.10.8)  Be sure to select the \u0026ldquo;Add to Path\u0026rdquo; option during the install process    Install the Python packages and VS Code extensions you need (see this page) Install Quarto CLI Quatro Instructions Start looking at Project 0 Complete the \u0026ldquo;Methods Checkpoint\u0026rdquo;  Installing Packages and Extensions Learn how to install packages by reading the assigned material and by watching the video tutorial on this page.\nThe readings mention a lot of different packages. For Project 0, you need to install at least pandas, altair, numpy, and jupyter.\nThe readings will also mention two VS Code extensions you need to install.\nA note on Jupyter Notebooks vs. Interactive Python Window The textbook will show you how to use VS Code\u0026rsquo;s interactive python windows and Quatro. Feel free to use Jupyter Notebooks.\nWe will do write-ups in Quarto, though, which can be rendered as a PDF or HTML\nIntroduction to Brother Comeau    What do you want to know?    What is a data scientist?    Brother Hathaway\u0026rsquo;s definition:\n A blend of programmer, statistician, and communicator that burns with curiosity.\n My definiton for DS 250:\n Someone who can extract insights from data and then communicate those insights with clarity.\n Learn more about the BYU-Idaho data science program here.\n   What is data science programming?    Data scientists write code as a means to an end, whereas software developers write code to build things. Data science is inherently different from software development in that data science is an analytic activity, whereas software development has much more in common with traditional engineering.\nData scientists tackle problems such as identifying fraudulent transactions, or predicting which employees are likely to leave a company. Software developers can take the data scientists models and turn them into fully functioning systems with production-quality code. Software developers tackle problems like getting an algorithm to run more efficiently, or building user interfaces.\n   Course Outcomes    Upon completing this course, you will be able to use data-driven programming in Python to handle, format, and visualize data. We will introduce you to data wrangling techniques (panadas), analytical methods (scikit-learn), and the grammar of graphics (Altair). Specifically, as a successful learner, you will be able to:\n Use functions, data structures, and other programming constructs efficiently to process and find meaning in data. Programmatically load data from various types of data sources, including files, databases, and remote services. Use data manipulation libraries to perform straightforward analysis, produce charts, and prepare data for machine learning algorithms. Use machine learning libraries to discover insights, make predictions, and interpret the success of these algorithms. Collaborate and share your work with industry-leading tools.     BYU-Idaho Mission Statement     Brigham Young University-Idaho was founded and is supported and guided by The Church of Jesus Christ of Latter-day Saints. Its mission is to develop disciples of Jesus Christ who are leaders in their homes, the Church, and their communities.\n  How would you describe a leader? What makes a leader powerful? What does a leader do with insights?  An example of a good leader.\nWhat (or who) is truth?\n   ## Course Format and Grading How hard is this class going to be?    The reality of CSE 250:\n We have done all we can to ensure that this is a 2-credit course for the average student. That means that we expect 4-6 hours outside of class for the average student to achieve an A. You have to put in the time if you want to build skills. The course is necessarily creative in nature. That fact usually makes it feel more challenging. We will be asking you to learn to write creative data science python code. If you have any concerns, please talk with me!     What is the structure of CSE 250?    The class uses 7 projects to teach data science programming in Python using pandas, Altair, scikit-learn, and numpy.\n Projects Syllabus     How do I get the grade I want?     Specification Grading Grading structure Competency Elements  Introduction Project \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026gt;\nWhat is the goal?    Completing the introduction project will set you up for success the rest of the semester. The workflow followed in the introduction project (loading packages, writing code, saving images, compiling a final report) will be the same for every other project . If you have questions about this project, you need to seek help.   What exactly do I need to submit?    Make sure you carefully read the project instructions.\nYou will submit a single .pdf file to I-Learn. This pdf file should contain an project summary, your answers to the grand questions (including the plot you saved with altair_saver), and an appendix where you copy and paste your commented Python code.\n   --------------------------------------------------------   ----------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/introduction\/day01\/"},{value:"Day 2: Commit, push, fork, and merge",label:"<p>Welcome to class! Announcements Practice with Git GQ3: add, commit, push and a little pull Let\u0026rsquo;s save the changes we\u0026rsquo;ve made to our resume.\nGQ4: Fork and merge Get into groups of 2 or 3. Then follow the steps below:\n fork the other student\u0026rsquo;s resume repository. Now clone that forked repository to your computer. On your local version of the forked repository, do the following:\nA. Create a new file called feedback.md B. Make a few recommendations or notes in the feedback.md file that will help the other student improve his or her resume\nC. add, commit, push your edits\nD. Go to the forked repo on GitHub and check if the feedback.md file shows up online Now, create a pull request to get your edits into the other student\u0026rsquo;s original repo.  Once you\u0026rsquo;ve given another student feedback, accept any pull requests submitted to your own repo. Continue to edit and improve your resume based on the feedback you received.\nGQ5: Fork into byuids-resumes Fork your own resume repository into the BYU-I Data Science Resumes group.\nIf you change your resume after you create this fork, you will have to submit a pull request to make sure the final version of your resume shows up in the group.\nThese instructions will help you create a pull request.\n</p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p6\/d3\/"},{value:"Day 3: Making your name stand out",label:"<p>Welcome to class! Reminder about resources  \u0026ldquo;Potluck\u0026rdquo; prep assignment Work with peers Make your own cheat sheet  Anouncements  Resume Workshop: 5:00-6:00 Tonight (Wed 1\/18) STC 375 Always submit a halfway checkpoint even if you\u0026rsquo;re behind!  It\u0026rsquo;s the only hard due date Think of it as a check in with \u0026ldquo;the boss\u0026rdquo;    Thoughts on P1 Halfway Checkpoint  Do your work in .py or .ipynb file, write-up in .qmd Making\/submitting a video: Loom alt.Save() Quarto  Let\u0026rsquo;s practice! Explore the data\nimport altair as alt import pandas as pd import numpy as np url = \u0026#34;https:\/\/github.com\/byuidatascience\/data4names\/raw\/master\/data-raw\/names_year\/names_year.csv\u0026#34; names = pd.read_csv(\u0026#39;names_year.csv\u0026#39;) names.head() names.describe() What do you want the chart to look like?\nWhat types of charts are there?\nalt.Chart() .mark_*() .encode() # alt.Chart().encode() \nWhat data do you need to make that chart?\n# names[[\u0026#39;name\u0026#39;],[\u0026#39;year\u0026#39;]] vs. names.query() kobe = names.query(\u0026#34;name == \u0026#39;Kobe\u0026#39;\u0026#34;)[[\u0026#34;name\u0026#34;, \u0026#34;year\u0026#34;, \u0026#34;Total\u0026#34;]] kobe2 = names.query(\u0026#34;name == \u0026#39;Kobe\u0026#39;\u0026#34;).filter(items=[\u0026#34;name\u0026#34;, \u0026#34;year\u0026#34;, \u0026#34;Total\u0026#34;]) # method chaining with () \nWork with your partner to create a line chart that includes both of your names?      Can you include total and data for the state in which you were born? Work together to make the code as eloquent as possible. compound charts      What can you add to your chart to help tell a story?\nCan you modify your previous chart to include your birth state?     Can you include Total and your birth state? Is there a better metric than raw counts that you could calculate? Are there good labels that you could include on the chart (mark_text())?     Remember this advice from Edward Tufte.\n To be truthful and revealing, data graphics must bear on the question at the heart of quantitative thinking: \u0026ldquo;Compared to what?\u0026rdquo; The emaciated, data-thin design should always provoke suspicion, for graphics often lie by omission, leaving out data sufficient for comparisons.\n What are some charts types we could use to answer this question?    There is a clear first choice, but I think there are a few other choices that could provide insight.\n  Visualization Catalog Altair Example Gallery       Use the query() method and filter() method to get your name and years in the rows with and include the name, year, and Total columns     filter the data down to your names (query) select the pertinent columns (filter()) Create a new data object for your name.     Create a line chart with your name.    base = (alt.Chart() .encode( x = alt.X(\u0026#39;\u0026#39;), y = alt.Y(\u0026#39;\u0026#39;) ) .mark_line() )    Create a new DataFrame with your birthday information in the row    Create a DataFrame with x, y, and label as columns. How to create a dataframe.   Add the vertical rule mark to show your birthday    These references can help:\n Using layered charts Altair Marks Add a horizontal line to an existent chart     Work with your partner to create a line chart that includes both of your names?      Can you include total and data for the state in which you were born? Work together to make the code as eloquent as possible.      Can you modify your previous chart to include your birth state?     Can you include Total and your birth state? Is there a better metric than raw counts that you could calculate? Are there good labels that you could include on the chart (mark_text())?     Now come up with a different chart than a line chart    Just use your state count or the Total count for your name.   --------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p1\/d3\/"},{value:"Day 3: Missing Data",label:"<p>Welcome to class! Announcements Questions 1 and 2 What issues are we still running into?\nHow to work with missing data What counts as missing data? How to identify missing data  df.isnull().sum() df.describe() df.column.value_counts(dropna=False)   pd.crosstab()  Option 1: Remove missing values Be careful with .dropna(), and make sure you know what it is doing to your data!\nLet\u0026rsquo;s use the pandas example:\ndf = pd.DataFrame({\u0026#34;name\u0026#34;: [\u0026#39;Alfred\u0026#39;, \u0026#39;Batman\u0026#39;, \u0026#39;Catwoman\u0026#39;], \u0026#34;toy\u0026#34;: [np.nan, \u0026#39;Batmobile\u0026#39;, \u0026#39;Bullwhip\u0026#39;], \u0026#34;born\u0026#34;: [pd.NaT, pd.Timestamp(\u0026#34;1940-04-25\u0026#34;), pd.NaT]})  Q: When would we ever use dropna()?    A: Almost never! Why do you think it is a bad idea? df.dropna()   Q: What argument do we use to drop rows where all values are NA?    A: df.dropna(how=\u0027all\u0027) reference   Q: What if we want to drop NA rows based on one column?    A: df.dropna(subset=[\u0027toy\u0027]) reference   Option 2: Replacing missing values Again, let\u0026rsquo;s use the pandas example:\ndf = pd.DataFrame([[np.nan, 2, np.nan, 0], [3, 4, np.nan, 1], [np.nan, np.nan, np.nan, 5], [np.nan, 3, np.nan, 4]], columns=list(\u0026#34;ABCD\u0026#34;))  Q: What if we want to replace all the NA in the wt column with the mean weight?    A: fillna() reference   Q: What if we want to replace all the 999 with a 4?    A: replace() reference   Q: What if we want to replace all the NAs with a linear interpolation?    A: interpolate() reference   Question 3 What columns do we need to use for question 3 (total number of flights delayed by weather)?  num_of_delays_weather num_of_delays_late_aircraft num_of_delays_nas  weather = flights.assign( severe = #????, mild_late = #????, mild_nas = np.where(#????), total_weather = # add up severe and mild, ).filter([\u0026#39;airport_code\u0026#39;,\u0026#39;month\u0026#39;,\u0026#39;severe\u0026#39;,\u0026#39;mild_late\u0026#39;,\u0026#39;mild_nas\u0026#39;, \u0026#39;total_weather\u0026#39;, \u0026#39;num_of_delays_total\u0026#39;]) Other resources for question 3  isin() method where() method Adding new variables with assign() assign() method  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p2\/d3\/"},{value:"Day 3: The end of baseball",label:"<p>Welcome to class! Spiritual Thought Announcements  Practice Coding Challenge Can I still get an \u0026ldquo;A\u0026rdquo;?  Profile of an \u0026ldquo;A\u0026rdquo; student What if I fall behind?   Reminders:  DS community assignment Review and Request Letter    Coding Challenge: How do I prepare? What would your coding challenge look like?\nProject 3 Questions  Integer Division Career Batting Average What have come up with for Q3? Metrics? Visualizations?  Question 1 Ask yourself:\n What do I want and expect the end table to look like? What table(s) and calculations do I need? What makes a row in my end table unique? What problems can I anticipate?  Question 2 Ask yourself:\n What do I want and expect the end table to look like? What table(s) and calculations do I need? What makes a row in my end table unique? What problems can I anticipate?  Question 3 What are some ideas for Grand Question 3? Ask yourself:\n What information will you use to compare the two baseball teams? What table(s) and calculations do I need? What makes a row in my end table unique? What problems can I anticipate?  and FROM -- JOIN -- ON -- WHERE -- GROUP BY -- ORDER BY -- LIMIT -- ``` -------------------------------------------  ## Connecting to SQLite: [Lahman SQLite](https:\/\/byuistats.github.io\/CSE250-Course\/data\/lahmansbaseballdb.sqlite) __Download the sqlite file:__ [Lahman sqlite](https:\/\/byuistats.github.io\/CSE250-Course\/data\/lahmansbaseballdb.sqlite) ### What is SQLite?  - [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/SQLite): SQLite is **a popular choice as embedded database software for local\/client storage in application software such as web browsers.** It is arguably the most widely deployed database engine, as it is used today by several widespread browsers, operating systems, and embedded systems (such as mobile phones), among others. SQLite has bindings to many programming languages.  - [SQLite.org](https:\/\/www.sqlite.org\/about.html): **SQLite is an in-process library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine.** The code for SQLite is in the public domain and is thus free for use for any purpose, commercial or private. SQLite is the most widely deployed database in the world with more applications than we can count, including several high-profile projects.  - [Codecademy](https:\/\/www.codecademy.com\/articles\/what-is-sqlite): SQLite is a database engine. It is software that allows users to interact with a relational database. In SQLite, a database is stored in a single file — a trait that distinguishes it from other database engines. This fact allows for a great deal of accessibility: copying a database is no more complicated than copying the file that stores the data, sharing a database can mean sending an email attachment. ### Working with SQLite files in Python ```python # %% import pandas as pd import altair as alt import numpy as np import sqlite3 # %% sqlite_file = \u0027lahmansbaseballdb.sqlite\u0027 con = sqlite3.connect(sqlite_file) # %% # See the tables in the database table = pd.read_sql_query( \u0022SELECT name FROM sqlite_master WHERE type=\u0027table\u0027\u0022, con) print(table) ``` ------------------------------------------------------ What table do we want to use?    q = \u0026#39;\u0026#39;\u0026#39; SELECT * FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What columns do we want to select?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What calculation do we want to perform?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r, r\/ab FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What name do we give our calculated column?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r, r\/ab as runs_atbat FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    #### I want to join two tables to help in decision making __For seasons after 1999, which year had the most players selected as All Stars but didn\u0027t play in the All Star game?__ - Provide a summary of how many games, hits, and at bats all the players had in that year\u0027s post season. - The [data dictionary](https:\/\/data.world\/byuidss\/cse-250-baseball-database\/workspace\/file?filename=readme2014.txt) might help. ```python import pandas as pd import altair as alt import numpy as np import datadotworld as dw baseball_url = \u0027byuidss\/cse-250-baseball-database\u0027 ``` What table do we want for All Star information?    # %% # allstar table dw.query(baseball_url, \u0026#39;\u0026#39;\u0026#39; SELECT * FROM AllstarFull WHERE --? AND --? LIMIT 5 \u0026#39;\u0026#39;\u0026#39;).dataframe    Can you use a groupby to get the counts of players per year?    dw.query(baseball_url, \u0026#39;\u0026#39;\u0026#39; SELECT yearid, -- \u0026lt;stuff to calculate\u0026gt; FROM AllstarFull WHERE yearid \u0026gt; 1999 AND gp != 1 GROUP BY --? ORDER BY --? \u0026#39;\u0026#39;\u0026#39;).dataframe    What table do we want for the post season at bats?    dw.query(baseball_url, \u0026#39;\u0026#39;\u0026#39; SELECT * FROM BattingPost as bp LIMIT 5 \u0026#39;\u0026#39;\u0026#39;).dataframe    Can you join the post season batting table and AllStar information?     For each player, keep only the at bats, hits, the all star gp, and gameid columns. Let\u0026rsquo;s only keep players with at least one at bat in the post season.  dw.query(baseball_url, \u0026#39;\u0026#39;\u0026#39; SELECT -- \u0026lt;columns to keep\u0026gt; FROM BattingPost as bp JOIN AllstarFull as asf ON -- \u0026lt;two columns for the join\u0026gt; WHERE bp.yearid \u0026gt; 1999 AND gp != 1 AND -- \u0026lt;at bat condition\u0026gt; LIMIT 15 \u0026#39;\u0026#39;\u0026#39; ).dataframe    Let\u0026rsquo;s build the final table    For seasons after 1999, which year had the most players selected as All Stars but didn\u0026rsquo;t play in the All Star game?\n Provide a summary of how many games, hits, and at bats all the players had in that year\u0026rsquo;s post season.  dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, \u0026#39;\u0026#39;\u0026#39; SELECT -- \u0026lt;lots of calculations\u0026gt; FROM BattingPost as bp JOIN AllstarFull as asf ON bp.playerid = asf.playerid AND bp.yearid = asf.yearid WHERE bp.yearid \u0026gt; 1999 AND gp != 1 AND ab \u0026gt; 0 GROUP BY -- \u0026lt;column\u0026gt; ORDER BY -- \u0026lt;column\u0026gt; \u0026#39;\u0026#39;\u0026#39; ).dataframe    --------------------------------------------------------------------- I want to see how much each college player from schools in the west and mountain west has made over their professional career. I want to know the full school name attended and the the Given name of each player. _Is this query correct?_ ```SQL SELECT cp.playerID, nameGiven, birthYear ,cp.schoolID, name_full ,SUM(salary) as salary FROM salaries as sal JOIN people as p ON p.playerID = sal.playerID JOIN CollegePlaying as cp ON p.playerID = cp.playerID JOIN schools as sc ON sc.schoolID = cp.schoolID WHERE sc.state = \u0027ID\u0027 GROUP BY cp.playerID, cp.schoolID ORDER BY name_full ``` ```python pd.read_sql_query( \u0027\u0027\u0027 SELECT cp.playerID, nameGiven, birthYear ,cp.schoolID, name_full ,SUM(salary) as salary FROM salaries as sal JOIN people as p ON p.playerID = sal.playerID JOIN CollegePlaying as cp ON p.playerID = cp.playerID JOIN schools as sc ON sc.schoolID = cp.schoolID WHERE sc.state = \u0027ID\u0027 GROUP BY cp.playerID, cp.schoolID ORDER BY name_full \u0027\u0027\u0027, con) ``` #### Let\u0027s start here ```python schools = pd.read_sql_query( \u0027\u0027\u0027 SELECT * FROM schools WHERE state = \u0027ID\u0027 \u0027\u0027\u0027, con) ``` ----------------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p3\/d3\/"},{value:"Day 3: Training a Classifier, Part 2",label:"<p>Welcome to class! Spiritual Thought Announcements  Coding Challenge code posted  Prepping data for the Machine Building a Decision Tree  Import packages    import pandas as pd import altair as alt from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn import metrics    Load and split data    # %% # Load data dwellings_ml = pd.read_csv(\u0026#34;https:\/\/github.com\/byuidatascience\/data4dwellings\/raw\/master\/data-raw\/dwellings_ml\/dwellings_ml.csv\u0026#34;) #%% # Separate the features (X) and targets (Y) x = dwellings_ml.filter([\u0026#34;livearea\u0026#34;,\u0026#34;basement\u0026#34;,\u0026#34;stories\u0026#34;,\u0026#34;numbaths\u0026#34;]) y = dwellings_ml[[\u0026#34;before1980\u0026#34;]] #%% Split the data into train and test sets x_train, x_test, y_train, y_test = train_test_split(x, y)    Build and evaluate a decision tree    #%% # Create a decision tree classifier_DT = DecisionTreeClassifier(max_depth = 4) # Fit the decision tree classifier_DT.fit(x_train, y_train) # Test the decision tree (make predictions) y_predicted_DT = classifier_DT.predict(x_test) # Evaluate the decision tree print(\u0026#34;Accuracy:\u0026#34;, metrics.accuracy_score(y_test, y_predicted_DT))    Understanding Your Model Visualizing decision trees  From the readings: A visual introduction to machine learning How to visualize a decision tree in python  #%% from sklearn import tree import matplotlib #%%  # method 1 - text print(tree.export_text(classifier_DT)) #%%  # method 2 - graph tree.plot_tree(classifier_DT, feature_names=x.columns, filled=True) \nPlotting feature importance  Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable. (link)\n What do we need from our model to create this plot?\nref\n#%%  # Feature importance classifier_DT.feature_importances_ #%% feature_df = pd.DataFrame({\u0026#39;features\u0026#39;:x.columns, \u0026#39;importance\u0026#39;:classifier_DT.feature_importances_}) feature_df \nEvaluating model performance Do your reading! Read How to evaluate your ML model and try googling other ideas.\n  The boy who cried wolf (the confusion matrix)\n  Accuracy\n  Precision and Recall\n  How to evaluate your ML model\n  Tour of Evaluation Metrics for Imbalanced Classification\n  Accuracy Problem 2 is looking for a model that has \u0026ldquo;at least 90% accuracy\u0026rdquo;.\nConfusion Matrix A confusion matrix is a quick way to see the strengths and weaknesses of your model. A confusion matrix is not a \u0026ldquo;metric\u0026rdquo;. A confusion matrix provides an easy way to calculate multiple metrics such as accuracy, precision, and recall.\nYour turn: Look at the confusion matrix for our Decison Tree model. Where the model is doing well and where it might be falling short?\n#%% # a confusion matrix print(metrics.confusion_matrix(y_test, y_predicted_DT)) #%% # this one might be easier to read print(pd.crosstab(y_test.before1980, y_predicted_DT, rownames=[\u0026#39;True\u0026#39;], colnames=[\u0026#39;Predicted\u0026#39;], margins=True)) #%% # visualize a confusion matrix # requires \u0026#39;matplotlib\u0026#39; to be installed metrics.plot_confusion_matrix(classifier_DT, x_test, y_test) </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p4\/d3\/"},{value:"Day 3: Validating data, cleaning columns",label:"<p>Welcome to class! Announcements Spiritual Thought Let\u0026rsquo;s validate some data! Pick something from the Star Wars article you want to validate (\u0026ldquo;double check\u0026rdquo;).\nMoving from categories to values.   Create an additional column(s) that converts the income ranges to a number. Create an additional column(s) that converts the age ranges to a number. Create an additional column(s) that converts the school groupings to a number.    str.replace(\u0027\u0026rsquo;, \u0026lsquo;9\u0026rsquo;) astype(\u0026lsquo;float\u0026rsquo;) pd.concat(axis=1)  Validating visuals You\u0026rsquo;re going to make a lot of bar charts!\n Simple bar chart tutorial. Make Altair do the counting for you! Tutorials here and here.  Getting started on Grand Question 3 One-hot encoding Project 5 asks you to \u0026ldquo;one-hot encode all columns that have categories\u0026rdquo; and \u0026ldquo;convert all yes\/no responses to 1\/0 numeric\u0026rdquo;.\nThe get_dummies method can be used to create one-hot encoded variables. The pd.get_dummies documentation is a great place to start.\nAfter reading the documentation, study the code below and get started on Grand Question #3.\n#%% # When we use machine learning to predict salary, # let\u0026#39;s only look at people that have seen at least # one star wars film starwars = starwars.query(\u0026#39;have_seen_any == \u0026#34;Yes\u0026#34;\u0026#39;) # Discuss - what\u0026#39;s a better way to filter out people  # who haven\u0026#39;t seen star wars? # %% # Format columns for machine learning # Let\u0026#39;s try this first: convert categories to \u0026#34;one-hot\u0026#34; encodings shot_first_onehot = pd.get_dummies(starwars.shot_first) shot_first_onehot # What the difference between code above, # and this? Which one is better? shot_first_onehot = pd.get_dummies(starwars.shot_first, drop_first=True) shot_first_onehot # %% # \u0026#39;get_dummies()\u0026#39; can also be used to convert yes\/no answers to 0\/1 episode_i = pd.get_dummies(starwars.seen_film_i__the_phantom_menace) episode_i # %% episode_i.value_counts() </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p5\/d3\/"},{value:"pandas and Altair",label:"<p>For this skill builder, we are exploring some important functions in the package of pandas and Altair. DS programming requires a lot of data wrangling. Using the proper functions, we can create concise and comprehensive codes. You should be exposed to a few functions through the readings this week.\nYou may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings. A prepared student should be able to finish the exercises within 60 minutes. You should work through it on your own.\nBefore you start Make sure you have installed VS-code, pandas, and Altair on your computer. You can install these packages by typing this line in the terminal:\npip install pandas altair\nOR if you have more than one version of python:\npip3.9 install pandas altair\npip3.9 indicates the version of python you are installing the packages to.\nData import Run the following code to import the data we need for this skill builder:\n# package import import numpy as np import pandas as pd import altair as al # data import dat = pd.read_csv(\u0026#34;https:\/\/vincentarelbundock.github.io\/Rdatasets\/csv\/AER\/Guns.csv\u0026#34;) Make sure the variable dat is correctly assigned in your environment and finish the following exercises. You can read the documentation of the data on this page - https:\/\/vincentarelbundock.github.io\/Rdatasets\/doc\/AER\/Guns.html\nExercise 1 One of the first things we can do to a freshly imported data is to check its columns. This will help us understand the basic structure of the dataframe(table).\n Using one line of code, select all the columns in dat, assign it to a variable called col_list.\n  Hint Every dataframe has an attribute \u0022columns\u0022. Accessing this attribute will give you a list of all column names  We often want to know the dimension of a dataframe. How many columns are in the dataset? How many rows are in the dataset?\n Using one line of code, show the number of columns and rows in dat.\n  Hint Every dataframe has an attribute \u0022shape\u0022. Accessing this attribute will give you the dimension of a datafarme  Now run dat.head(). It will print out the first 5 rows of data in dat.\n Just from looking at the output, what column(s) seems to be redundant with the row number?\n  Hint There is one column that serves as nothing but a row counter, that columns is redundant.  Exercise 2 After a brief investigation of the data, we will clean up the data. By cleaning up, we are trying to filter down dat so this only holds data we need. We will first get rid of the extra column we found in the previous excercise.\n Using one line of code, drop the redundant column using the variable col_list (created in excercise 1)\n  Hint Use `drop()`. Understand what \u0026ldquo;axis\u0026rdquo; is as a parameter of drop().\nYour function should looks like this:\ndat.drop([col_list[_]], axis = _)\nfill the \u0026ldquo;_\u0026quot;\u0026rsquo;s with the correct values and assign the output to dat.\n Don\u0026rsquo;t forget to save the changes in dat. Run dat.head() to make sure the column is dropped in dat.\nExercise 3 We have filtered dat vertically by dropping a column. Now we will try to filter dat horizontally, meaning we will get rid of some the rows.\nWe can do that by applying a condition to dat. A condition is an expression that can be evaluated as True\/False. For example, 8 \u0026gt; 5 is an expression that evaluates to be True. This is trivial because 8 will always be greater than 5.\nRun the code below:\n what is the difference between exp1 and exp2?\n exp1 = 8 \u0026gt; 5 exp2 = dat.violent \u0026lt; 300  Hint Try type() on else variable OR calling else variable.  Run ths code below:\n By putting dat.violent \u0026lt; 300, and the violent column from dat into a dataframe, what is the relationship between the two columns?\n exp = pd.DataFrame({\u0026quot;dat.violent \u0026lt; 300\u0026quot; : exp2, \u0026quot;violent value from dat\u0026quot; : dat.violent}) exp  Hint Try computing `dat.violent[n]  Using query(), filter down the dat so that it only contains the data for idaho\n  Hint query() takes in expressions and filters down data.  Don\u0026rsquo;t forget to save the changes in dat. Run dat.shape() to make sure the there are 23 rows and 13 columns.\nExercise 4 Besides filtering, we can manipulate the data by adding new data to it. By adding a new column to the data, we assign a new value to each row.\n Using assign(), create a new column that show the ratio between murder rate and violent rate.\n  Hint Use assign() You see get the ratio by computing this code:\ndat.murder\/dat.violent\n Exercise 5  Create a scatter plot that shows the relationship between murder rate and violent rate for the state of Idaho. Your chart should show murder rate as the x-axis, violent as the y-axis.\n  Hint Can you mimic this plot? (https:\/\/altair-viz.github.io\/gallery\/scatter_tooltips.html)\n  For an extra push Exercise 6  Using a line of code, filter down the data set so that it only shows the data in years between 1993 and 1997.\n Exercise 7  Create a line chart that show prisoners numbers for the state of Idaho, Utah, and Oregon.\n Your chart should show year as the x-axis, prisoner as the y-axis, states as different colours, along with an appropriate title.\nExercise 8  Without using query(), finshed the data wrangling in question 2,5 and 6.\n After you have completed this skill builder with your team (or on your own) then compare your work to our script    See the script.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/pandas_altair\/"},{value:"Project 1: What\u0027s in a name?",label:"<p>Background Early in prehistory, some descriptive names began to be used again and again until they formed a name pool for a particular culture. Parents would choose names from the pool of existing names rather than invent new ones for their children.\nWith the rise of Christianity, certain trends in naming practices manifested. Christians were encouraged to name their children after saints and martyrs of the church. These early Christian names can be found in many cultures today, in various forms. These were spread by early missionaries throughout the Mediterranean basin and Europe.\nBy the Middle Ages, the Christian influence on naming practices was pervasive. Each culture had its pool of names, which were a combination of native names and early Christian names that had been in the language long enough to be considered native. [ref]\nData Download: names_year.csv\nInformation: data.md\nReadings  Python for Data Science (P4DS): Data Visualization P4DS: Graphics for Communication P4DS: Markdown P4DS: 5.2 Filter rows with .query() P4DS: Chapter 10 DataFrame  Optional References  The query method  Questions and Tasks For Project 1 the answer to each question should include a chart and a written response. The years labels on your charts should not include a comma. At least two of your charts must include reference marks.\n How does your name at your birth year compare to its use historically? If you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess? Mary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice? Think of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?  Deliverables Use this template to submit your Client Report. The template has three sections (for additional details please see the instructional template):\n A short summary that highlights key that describes the results describing insights from metrics of the project and the tools you used (Think “elevator pitch”). Answers to the grand questions. Each answer should include a written description of your results, code snippets, charts, and tables.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/project-1\/"},{value:"Python for Data Science",label:"<p>Python for Data Science is a port of R for Data Science into Python. We are keeping Garrett Grolemund and Hadley Wickham’s writing and examples as much as possible while demonstrating Python instead of R. We have focused on pandas and Altair in our Python code snippets.\nThis book will teach you how to do data science with Python: You’ll learn how to get your data into Python, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with Python. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.\nInstalling and Importing Packages We want to install the following three packages;\n pandas numpy scikit-learn. The Apple Silicon is still more difficult to get installed. You can use the following links to get it installed - Link 1, Link 2, Link 3.  We can get packages installed for this course using one of the two methods below.\nUsing your terminal # default way pip install numpy pandas scikit-learn If you are using a Mac\n# Mac method with Python 2 and 3 installed pip3 install numpy pandas scikit-learn Using your interactive Python (Jupyter server) import sys !{sys.executable} -m pip install numpy pandas scikit-learn    </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/python-for-data-science\/"},{value:"Week 10-11: Project 5 - Star Wars",label:"<p> A significant portion of a data scientist\u0026rsquo;s job is data cleaning. during these two weeks we will not hide the data munging from you. We will practice data cleaning using a Star Wars survey from FiveThirtEight. Survey data is notoriously difficult to handle. Even when the data is recorded cleanly the options for ‘write in questions’, ‘choose from multiple answers’, ‘pick all that are right’, and ‘multiple choice questions’ makes storing the data in a tidy format difficult.\n </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p5\/"},{value:"Day 1: Git and Github",label:"<p>Welcome to class! Spiritual Thought Announcements   Project 5 Comment\n Feature Importance and Model discussion    The last day of DSS is next Wednesday, Mar 29th at 6:00PM in STC 394\n  Extra credit for creating and uploading cheat sheet (2 points for projects or checkpoints)\n  Coding Challenge date?\n  The technical aspects of Project 6 will be done mostly in class. Resume prep\/MD outside\n  Git and GitHub \u0026ldquo;Web developers\u0026rsquo; social media platform\u0026rdquo;  This is GitHub, the world’s largest code repository platform online. A platform used by some 50 million software developers to host their coding projects, most of them open-source — meaning others can access their codes and modify them to create better versions if they feel like.\nMost of the internet is produced or hosted on GitHub in the form of code. “What Gmail is to email, GitHub is to writing software,” says Kiran Jonnalagadda, cofounder of HasGeek, a platform to build and discover peer groups. Source\n  Don\u0026rsquo;t: post code for assignments that hundreds of other students have done. Do: post unique code using skills from your classes.  I would also recommend using private repos to manage your course work.\nIs it going to hurt? Answer: Yes.\nIt feels weird at first but quickly becomes second nature. If you plan on taking more data science classes, you should know that DS 350 students are required to submit all coursework via GitHub. This is a major topic in class and office hours for the first two weeks. Then we practically never discuss it again.\nMore bad news. Do you use GitHub to work with other people or to coordinate your own work from multiple computers? If so, after you recover from the initial setup, Git will crush you again with merge conflicts. And this is not one-time pain, this could be a dull ache for a long time.\n Managing a project via Git\/GitHub is much like the Google Doc scenario and enjoys many of the same advantages. It is definitely more complicated than collaborating on a Google Doc, but this puts you in the right mindset. Source\n Step 1: Download and install Follow steps 1-4 of this tutorial.\nThen:\n Request access tothe BYU-I Resumes page at Request Access Respond to the auto-generated email Wait a few minutes for authorization Join our GitHub organization - byuids-resumes.  If you are on a Mac, you may need:  Mac fix with paths Download Xcode and update (10 gig download) VSCode path selection (scroll down to step 1)  Step 2: Create a repository from the resume template and connect to the BYUI Step 3: Publish your resume to GitHub Pages  Go to settings for your repo. Scroll down to the GitHub Pages section. Under source select the box which says None and pick master. Now select the \/docs folder and click save. Copy your site URL at the top of the \/settings\/pages location. Add your link to the About section of your repository. Edit the readme.md in the base repo to not show the resume directions.  Step 4: Clone repo into VS Code Analytics Vidhya reading\nStep 5: Make your resume look good Examples:\n Undergraduate DS resumes Hathaway\u0026rsquo;s resume  You may also find these articles helpful:\n How to Write a Great Data Science Resume How to Build an Effective Data Science Resume How to Write the Perfect Data Scientist Resume  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p6\/d2\/"},{value:"Day 2: Intro to Machine Learning",label:"<p>Welcome to class! Announcements Spiritual thought Are facts true?  How do you distinguish between truth and error? Joshua and Caleb  Splitting the Data 1. Start with packages and data set We\u0026rsquo;ll be using some parts of SKLEARN package and the Seaborn package.\n# If you haven\u0026#39;t already, install scikit-learn and seaborn pip install scikit-learn seaborn from types import GeneratorType import pandas as pd import altair as alt import numpy as np import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.tree import DecisionTreeClassifier from sklearn import metrics What is the difference between dwellings_denver.csv and dwellings_ml.csv?\n2. Choose which variables to use How do we know which variables to use out of dwellings_ml.csv?\nQuestion 1 will help you identify patterns (or lack of patterns) in the data.\n3. Separate into features and target Which Features? # %% h_subset = dwellings_ml.filter([\u0026#39;livearea\u0026#39;, \u0026#39;finbsmnt\u0026#39;, \u0026#39;basement\u0026#39;, \u0026#39;yearbuilt\u0026#39;, \u0026#39;nocars\u0026#39;, \u0026#39;numbdrm\u0026#39;, \u0026#39;numbaths\u0026#39;, \u0026#39;stories\u0026#39;, \u0026#39;yrbuilt\u0026#39;, \u0026#39;before1980\u0026#39;]).sample(500) sns.pairplot(h_subset, hue = \u0026#39;before1980\u0026#39;) corr = h_subset.drop(columns = \u0026#39;before1980\u0026#39;).corr() # %% sns.heatmap(corr) 4. Split into training and testing sets What does the \u0026ldquo;train_test_split()\u0026rdquo; function do? x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = #???, random_state = #???) Read the documentation and tell me what is returned?\nFunction documentation\n Why do we use \u0026ldquo;test_size\u0026rdquo; and \u0026ldquo;random_state\u0026rdquo;?\n  What is \u0026ldquo;x\u0026rdquo; and \u0026ldquo;y\u0026rdquo; in the above function example?\n We need to take our data and build the feature and target data objects.\n What columns should we remove from our features (X)?\n  What column should we use as our target (y)?\n x = dwellings_ml.filter([#what variables will you use as \u0026#34;features\u0026#34;?]) y = dwellings_ml[#what variable is the \u0026#34;target\u0026#34;?] \nTraining a Classifier Decision Tree Example # create the model classifier = DecisionTreeClassifier() # train the model classifier.fit(x_train, y_train) # make predictions y_predictions = classifier.predict(x_test) # test how accurate predictions are metrics.accuracy_score(y_test, y_predictions) How to Improve Accuracy To improve the accuracy of your model, you could:\n Change what variables are used in the features (x) data set Change what type of model you are using Tune (aka, \u0026ldquo;change\u0026rdquo; or \u0026ldquo;tweak\u0026rdquo;) the parameters of the model  Other Classification Models Here are some other models you could try.\nfrom sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import GradientBoostingClassifier \nMake Progress on Project 4 Do the project readings    Machine Learning Introduction\n Step-by-step guide (mostly) for training a GaussianNB classifier. (The steps will be the same for any algorithm you use.)  Visual Introduction to Machine Learning\n Machine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. You can use it to make predictions. One method for making predictions is called a decision trees, which uses a series of if-then statements to identify boundaries and define patterns in the data. Overfitting happens when some boundaries are based on distinctions that don\u0026rsquo;t make a difference. You can see if a model overfits by having test data flow through the model.     Start working on Question 1    The goal of Grand Question 1 is to help us with \u0026ldquo;feature selection\u0026rdquo;.\n \u0026ldquo;Overfitting\u0026rdquo; happens when some boundaries are based on on distinctions that don\u0026rsquo;t make a difference. More data does not always lead to better models. (Occam\u0026rsquo;s Razor)  Common questions:\n Why it may be better to have fewer predictors in Machine Learning models? What is Feature Selection and why do we need it in Machine Learning?     What is the 5000 rows error with Altair?    The best way around this is to look at a sub-sample of the data for exploratory purposes. For example, you can use \u0026ldquo;sample(500)\u0026rdquo;. But there are ways to expand VS Code\u0026rsquo;s limits.\nMaxRowsError: How can I plot Large Datasets?\nYou may also save data to a local filesystem and reference the data by file path. Altair allows you to disable the max rows:\nalt.data_transformers.disable_max_rows() subset_data = denver.sample(n = 4999)    scikit-learn resources     Home page Tutorials Getting Started: What do you notice about the header portion of each of the script chunks?  import vs from ... import       My favorite comic    xkcd\n   ## Searching for patterns What ideas do you have for charts? ## Understanding the data What differences do you notice between these two data sets? ```python dwellings = pd.read_csv() dwellings_ml = pd.read_csv() ``` ------------------------------------------------------------------- What is the 5000 rows error with Altair?    MaxRowsError: How can I plot Large Datasets?\nYou may also save data to a local filesystem and reference the data by file path. Altair has a JSON data transformer that will do this transparently when enabled:\nalt.data_transformers.disable_max_rows() subset_data = denver.sample(n = 4999)    What features of homes might have changed a bit over time?    Some ideas:\n square footage number of bathrooms basement size  Let\u0026rsquo;s create one chart using some of these variables.\n   ----------------------------------------- What is scikit-learn?     Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n About scikit-learn helps us see the history and funding. It should stay \u0026ldquo;king of the hill\u0026rdquo; for a long time.\n Simple and efficient tools for predictive data analysis Accessible to everybody, and reusable in various contexts Built on NumPy, SciPy, and matplotlib Open source, commercially usable - BSD license     Should I import scikit-learn?    scikit-learn is very large, with many submodules. To help the user of your .py script understand your code, the consensus is to use from .... import .....\nfrom sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeClassifier from sklearn import metrics    After choosing a machine learning method, what do we do?     Fit (or \u0026ldquo;train\u0026rdquo;) the model using the features (also called \u0026ldquo;X\u0026rdquo;) Predict the target (also called \u0026ldquo;y\u0026rdquo;) Evaluate model performance (using many different metrics)     ## Train the model What does the train_test_split() function do?    Your turn: Read the documentation and tell me what is returned from the train_test_split() function.\nHow to save the output: Use a destructuring assignment\nx_train, x_test, y_train, y_test = train_test_split( x, y, test_size = .3, random_state = 76) Your turn:\n Why would we want to use the test_size and random_state arguments? What is x and y in the above example? Why do we care about splitting our data?     The next step    We need to take our data and build the feature and target data objects. Think about:\n What column(s) should we remove from our features (x)? What column(s) should we use as our target (y)?     ## Predicting targets and evaluating model performance What metrics should we use?    Do your reading! Read How to evaluate your ML model and try googling other ideas.\nAccuracy Question 2 is looking for a model that has \u0026ldquo;at least 90% accuracy\u0026rdquo;.\nConfusion Matrix A confusion matrix is a quick way to see the strengths and weaknesses of your model.\nYour turn: Look at the confusion matrix for our GaussianNB model. Where the model is doing well and where it might be falling short?\nYour turn: Now look at the confusion matrix for our Decision Tree model. What differences do you notice?\n# a confusion matrix print(metrics.confusion_matrix(y_test, y_predicted_GNB)) # this one might be easier to read print(pd.crosstab(y_test.flatten(), y_predicted_GNB, rownames=[\u0026#39;True\u0026#39;], colnames=[\u0026#39;Predicted\u0026#39;], margins=True)) # visualize a confusion matrix # requires \u0026#39;matplotlib\u0026#39; to be installed metrics.plot_confusion_matrix(classifier_GNB, x_test, y_test)    ------------------------------------------------------------------------- AI is able to learn \u0027rules\u0027 from highly repetitive data. [Sebastian Thrun](https:\/\/www.youtube.com\/watch?v=ZJixNvx9BAc)  The single most important thing for AI to accomplish in the next ten years is to free us from the burden of repetitive work. [Sebastian Thrun](https:\/\/www.youtube.com\/watch?v=ZJixNvx9BAc)   ### [Visual Introduction to Machine Learning](http:\/\/www.r2d3.us\/visual-intro-to-machine-learning-part-1\/)  1. Machine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. You can use it to make predictions.  2. One method for making predictions is called a decision trees, which uses a series of if-then statements to identify boundaries and define patterns in the data.  3. Overfitting happens when some boundaries are based on distinctions that don\u0027t make a difference. You can see if a model overfits by having test data flow through the model. #### [Bias-Variance Tradeoff](http:\/\/www.r2d3.us\/visual-intro-to-machine-learning-part-2\/)  1. Models approximate real-life situations using limited data.  2. In doing so, errors can arise due to assumptions that are overly simple (bias) or overly complex (variance).  3. Building models is about making sure there\u0027s a balance between the two. #### But what is the \u0027Pavlovian bell\u0027 in the machine learning model? ![](..\/..\/images\/ml\/test.png) Some mathematical penalty\/reward equation.  - __[Regression](https:\/\/setosa.io\/ev\/ordinary-least-squares-regression\/)__  - __[Variance, RMSE, SD](..\/..\/interactive\/threshold_histogram.html)__  - __proportions__ ## Using our project data to understand features, targets, and samples.  1. Import `dwellings_ml.csv` and write a short sentence describing your data. Remember to explain an observation and what measurements we have on that observation.  2. Now try describing the modeling (machine learning) we are going to do in terms of features and targets.  A. Are there any columns that are the target in disguise?  B. _Are the observational units unique in every row?_ ![](..\/..\/images\/ml\/iris_description.png) ### If your model is near perfect in its predictability, you might be cheating. ### Watch out for [transactional data](http:\/\/localhost:1313\/CSE250-Course\/images\/ml\/iris_description.png)!  - Financial: orders, invoices, payments  - Work: plans, activity records  - School: Grades ### [scikit learn](https:\/\/scikit-learn.org\/stable\/)  - [Tutorials](https:\/\/scikit-learn.org\/stable\/tutorial\/index.html)  - [Getting Started](https:\/\/scikit-learn.org\/stable\/getting_started.html): _What do you notice about the header portion of each of the script chunks?_  - [`import` vs `from ... import`](https:\/\/scikit-learn.org\/stable\/getting_started.html) ## Setting up Live Share ----------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p4\/d2\/"},{value:"Day 2: Seeing names with Altair",label:"<p>Welcome to class! Announcements Project Submissions  Don\u0026rsquo;t leave example text\/documentation from the Template in your writeup Change the Project Title (don\u0026rsquo;t have to call it Client Report) Code can be adjacent to the relevant output as long as it\u0026rsquo;s not distracting, but please include your complete code in an Appendix Be sure to save the QMD file before rendering   Autosave  Project 0 Wrap-up  If you still cannot render a document in Quarto, let me know Is python, at least up and running? able to plot graphs and make tables? Finishing up a report   Markdown  Tables - want to have the printed table in Markdown area, not a code area   HTML submissions  Other hints:\n Tutoring Lab Slack Channel: #tutoring_lab  Back to Day 1 Slides Methods Checkpoint Loading the names data    Visit the Project 1 Instructions to download the data. #%% # load packages import pandas as pd import altair as alt #%% # load data from url url = \u0026quot;this_is_the_url_to_the_csv_file\u0026quot; names = pd.read_csv(url) #%% # or, you can load data from file names2 = pd.read_csv(\u0026quot;names_year.csv\u0026quot;)    Pandas and DataFrames    What is a Pandas DataFrame? DataFrames come with attributes and built-in functions that can help us get a feel for our data.\nRun the code below one line at a time (or use other functions of your choice) to explore the names data. What do you learn?\nnames.columns names.shape names.size names.head() names.describe()    Understanding your data    You should be able to introduce your data sets to people, the same way you introduce a friend.\n If you can\u0026rsquo;t describe what a row is in your data, then you don\u0026rsquo;t understand what groups you can analyze. If you can\u0026rsquo;r describe what a column is in your data, then you don\u0026rsquo;t understand what information you can evaluate for each group.  Being able to explain your data out loud to someone else follows the same principles as rubber duck debugging.\n   Let\u0026rsquo;s practice!    Understanding column values How many unique names does the names dataset contain? Work with a partner to find the answer. I recommend searching the Pandas cheat sheet.\n pull the name column out as a series Use the pandas unique function pd.unique() find the size of the series  What is the range of years in the names dataset? Again, work with a partner and use the Pandas cheat sheet.\n pull the year column as a series Find the max Find the min     ----------------------------------------------------- How many unique years do we have for our name?    pd.unique(dat.query(\u0027name == \u0026quot;John\u0026quot;\u0027).year).min() pd.unique(dat.query(\u0027name == \u0026quot;John\u0026quot;\u0027).year).max() pd.unique(dat.query(\u0027name == \u0026quot;John\u0026quot;\u0027).year).size     Filtering rows of a DataFrame    Make sure to do the project readings!  P4DS: 5.2 Filter rows with .query() The query method     ------------------------------------ Getting started with Altair Why are we using Altair? It is built on the VEGA and D3 which are fast and web based.  Grammar of Graphics: Vega-Lite   Technical Paper Website Endorsment   Grand Grand Question 1 What does a chart need to look like to answer Question 1?\nWhat data do we need to build that chart?\nMaking our chart look good.  Size of chart Title and subtitle Size and color of line Axis formatting Reference marks  Extra Practice Altair (and Vega and Vega-Lite and D3!)    What is the difference between a \u0026ldquo;high-level\u0026rdquo; and \u0026ldquo;low-level\u0026rdquo; programming language or tool? Here\u0026rsquo;s what Google has to say.\n Altair is a Python library built on Vega and Vega-Lite Vega is a \u0026ldquo;higher-level visualization specification language on top of D3\u0026rdquo; that creates charts with json files D3 is a JavaScript library     Altair: Removing commas from years    Remember, Altair builds on Vega, which builds on D3. Sometimes to answer a question about Altair, you will have to read Vega or D3 documentation. For example:\n Altair\u0026rsquo;s guide for customizing axis labels. (Scroll down to the second code example.) D3 options for different axis formats.  (alt.Chart(my_data) .mark_line() .encode( x = alt.X(\u0027year\u0027, axis = alt.Axis(format = \u0027d\u0027, title = \u0026quot;Year\u0026quot;)), y = alt.Y(\u0027Total\u0027, axis = alt.Axis(title = \u0026quot;Children with Name\u0026quot;)) ) )    Altair: Adding a reference line    You may want to include a point or line of reference to help your chart answer the question \u0026ldquo;compared to what?\u0026rdquo;. Let\u0026rsquo;s say you have your chart for Grand Question 1 saved as question_1. The easiest way I have found to add a reference line is to create a new DataFrame with a single number:\nline_df = pd.DataFrame({\u0027year\u0027: [1990]}) line_df And use the new DataFrame to create a chart with a single line that has a specific value of x (for example, your birth year) but spans the entire y-axis.\nIn Altair, this is done with the the mark_rule() geometry. You can then \u0026ldquo;layer\u0026rdquo; the two charts together.\nline = alt.Chart(line_df).mark_rule(color=\u0026quot;red\u0026quot;).encode(x = \u0026quot;year\u0026quot;) final_chart = question_1 \u002b line final_chart Additional references:\n Using layered charts Altair Marks Add a horizontal line to an existent chart     Look at the names data and write a short paragraph in your notes describing the data set    We have a row for each name-year. Excluding the name and year columns we have a column for each state and DC. Finally there is a Total column that sums over the other columns.\n  If you can\u0026rsquo;t describe what a row is in your table then you don\u0026rsquo;t understand what groups you can talk about with your data. The columns tell you what information you will be able to evaluate on each \u0026lsquo;group\u0026rsquo; or \u0026lsquo;observation\u0026rsquo; in your data.   We want tidy data.\n   ----------------------- Which name has been given the most and the least?      Sum all the years for each name (groupby()). Create a new DataFrame for the totals. Write a query that filters the total data to the max and min. Create a markdown table with the information. A. to_markdown() requires the tabulate package. B. to_markdown() with arguments showindex and floatformat C. Guidance on floatformat   dat_total = dat.groupby(\u0027name\u0027).agg(n = (\u0027Total\u0027, \u0027sum\u0027)).reset_index() print(dat_total .query(\u0027n in [@dat_total.n.max(), @dat_total.n.min()]\u0027) .to_markdown(showindex = False, floatfmt=\u0026quot;.0f\u0026quot;))    -------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p1\/d2\/"},{value:"Day 2: SQL Calculations",label:"<p>Welcome to class! Spiritual Thought Announcements  Project 3 - SQL practice  Class Activity in Slack Part 1 Goal: Describe in words (NOT using code) how to get from your starting data to your ending data.\nPost your answer in your group\u0026rsquo;s Slack thread. You have 7 minutes, and are allowed to ask me 1 question.\nPart 2 Goal: Now try to write a SQL query to get your ending data.\nPost your SQL query in your group\u0026rsquo;s Slack thread. You have 7 minutes, and are allowed to ask me 1 question.\nHere is the SQL template for your use.\nSELECT -- \u0026lt;columns\u0026gt; and \u0026lt;column calculations\u0026gt; FROM -- \u0026lt;table name\u0026gt;  JOIN -- \u0026lt;table name\u0026gt;  ON -- \u0026lt;columns to join\u0026gt; WHERE -- \u0026lt;filter condition\u0026gt; GROUP BY -- \u0026lt;subsets for column calculations\u0026gt; HAVING -- \u0026lt;grouped filter condition\u0026gt; ORDER BY -- \u0026lt;how the output is returned in sequence\u0026gt; LIMIT -- \u0026lt;number of rows to return\u0026gt; \n- Group 1: [SELECT and FROM](https:\/\/docs.data.world\/documentation\/sql\/concepts\/basic\/SELECT_and_FROM.html) with the `people` table (called \u0022master\u0022 in the data dictionary). Include examples of `SELECT AS` and `SELECT DISTINCT`.  - Group 2: [WHERE](https:\/\/docs.data.world\/documentation\/sql\/concepts\/basic\/WHERE.html) with the `schools` table. Try using different types of comparison operators, or making multiple comparisons with `AND`.  - Group 3: [ORDER BY](https:\/\/docs.data.world\/documentation\/sql\/concepts\/basic\/ORDER_BY.html) with the `salaries` table. Try sorting in different orders (ascending or descending) and with multiple columns.  - Group 4: [JOIN](https:\/\/docs.data.world\/documentation\/sql\/concepts\/intermediate\/Joins.html) with the `schools` and `collegeplaying` tables (focus on \u0022inner\u0022 joins).  - Group 5: [Aggregations](https:\/\/docs.data.world\/documentation\/sql\/concepts\/intermediate\/aggregations.html) with the `batting` table.  - Group 6: [GROUP BY](https:\/\/docs.data.world\/documentation\/sql\/concepts\/intermediate\/GROUP_BY.html) with the `batting` table. -------------------------- Getting started Question One: Write an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID\/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nThink about:\n What tables (data) do you need? What SQL commands do you need?  What table do we want to use?    q = \u0026#39;\u0026#39;\u0026#39; SELECT * FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What columns do we want to select?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What calculation do we want to perform?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r, ab\/r FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; batting_calc = dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What name do we give our calculated column?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r, ab\/r as runs_atbat FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; batting_calc = dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    #### I want to join two tables to help in decision making __Which year had the most players players selected as All Stars but didn\u0027t play in the All Star game after 1999?__ - __provide a summary of how many games, hits, and at bats occured by those players had in that years post season.__ ```python import pandas as pd import altair as alt import numpy as np import datadotworld as dw con_url = \u0027byuidss\/cse-250-baseball-database\u0027 ``` What table do we want for All Star information?    # %% # allstar table dw.query(con_url, \u0026#39;\u0026#39;\u0026#39; SELECT * FROM AllstarFull WHERE AND LIMIT 5 \u0026#39;\u0026#39;\u0026#39;).dataframe    Can you use a groupby to get the counts of players per year?    dw.query(con_url, \u0026#39;\u0026#39;\u0026#39; SELECT yearid, -- \u0026lt;stuff to calculate\u0026gt; FROM AllstarFull WHERE yearid \u0026gt; 1999 AND gp != 1 GROUP BY --? ORDER BY --? \u0026#39;\u0026#39;\u0026#39;).dataframe    What table do we want for the post season at bats?    dw.query(con_url, \u0026#39;\u0026#39;\u0026#39; SELECT * FROM BattingPost as bp LIMIT 5 \u0026#39;\u0026#39;\u0026#39;).dataframe    Can you join the batting table and AllStar information and keep only the at bats, hits with the all star gp and gameid columns?    Let\u0026rsquo;s only keep players with at least one at bat in the post season\ndw.query(con_url, \u0026#39;\u0026#39;\u0026#39; SELECT -- \u0026lt;columns to keep\u0026gt; FROM BattingPost as bp JOIN AllstarFull as asf ON -- \u0026lt;two columns for the join\u0026gt; WHERE bp.yearid \u0026gt; 1999 AND gp != 1 AND -- \u0026lt;at bat condition\u0026gt; LIMIT 15 \u0026#39;\u0026#39;\u0026#39; ).dataframe    Let\u0026rsquo;s build the final table    Which year had the most players players selected as All Stars but didn\u0026rsquo;t play in the All Star game after 1999?\n provide a summary of how many games, hits, and at bats occured by those players had in that years post season.  dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, \u0026#39;\u0026#39;\u0026#39; SELECT -- \u0026lt;lots of calculations\u0026gt; FROM BattingPost as bp JOIN AllstarFull as asf ON bp.playerid = asf.playerid AND bp.yearid = asf.yearid WHERE bp.yearid \u0026gt; 1999 AND gp != 1 AND ab \u0026gt; 0 GROUP BY -- \u0026lt;column\u0026gt; ORDER BY -- \u0026lt;column\u0026gt; \u0026#39;\u0026#39;\u0026#39; ).dataframe    --------------------------------------------------------- Extra Practice \u0026ldquo;I get SQL and want to be challenged.\u0026rdquo; Do this Math 335 task with SQL commands in Python.\n</p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p3\/d2\/"},{value:"Day 2: Star Wars and strings",label:"<p>Welcome to class! Announcements What\u0026rsquo;s something you\u0026rsquo;re grateful for today? The .str functions in pandas   .str.strip: Strip white space .str.replace: replace one string of characters with another. .str.split: Separate a character string into two values. .str.join: Join two lists together Python for Data Science: Strings Pandas Documentation   .str.strip() s = pd.Series([\u0026#39;1. Ant. \u0026#39;, \u0026#39;2. Bee!\\n\u0026#39;, \u0026#39;3. Cat?\\t\u0026#39;, \u0026#39;4. Beat?\\t\u0026#39;, np.nan]) s.str.strip() s.str.strip(\u0026#39;123.!? \\n\\t\u0026#39;) s.str.strip(\u0026#39;1234.!? \\n\\t\u0026#39;) \n.str.replace() s.str.replace(\u0026#39;Ant.\u0026#39;, \u0026#39;Man\u0026#39;) s.str.replace(\u0026#39;a\u0026#39;, 8) s.str.replace(\u0026#39;a\u0026#39;, \u0026#39;8\u0026#39;) s.str.replace(\u0026#39;a\u0026#39;, \u0026#39;8\u0026#39;, case = False) s.str.replace(\u0026#39;a|e\u0026#39;, \u0026#39;8\u0026#39;, case = False) s.str.replace(\u0026#39;\\d\u0026#39;, \u0026#39;\u0026#39;, case = False) \n.str.split() s2 = pd.Series([\u0026#39;1-20\u0026#39;, \u0026#39;21-50\u0026#39;, \u0026#39;51-80\u0026#39;, \u0026#39;81-100\u0026#39;, np.nan]) s3 = pd.Series( [ \u0026#34;this is a regular sentence\u0026#34;, \u0026#34;https:\/\/docs.python.org\/3\/tutorial\/index.html\u0026#34;, np.nan ] ) s2.str.split() s3.str.split() s2.str.split(pat=\u0026#34;-\u0026#34;) \n.str.join() or .str.cat() two_columns = s2.str.split(\u0026#34;-\u0026#34;, expand = True).rename( columns = {0: \u0026#39;minimum\u0026#39;, 1: \u0026#39;maximum\u0026#39;}) two_columns.fillna(\u0026#34;\u0026#34;).agg(\u0026#34;__\u0026#34;.join, axis = 1) two_columns.minimum.str.cat(two_columns.maximum, sep = \u0026#34;__\u0026#34;) \nFixing the column names Here is some code to get you started:\nurl = \u0027https:\/\/github.com\/fivethirtyeight\/data\/raw\/master\/star-wars-survey\/StarWars.csv\u0027 starwars_data = pd.read_csv(url, encoding = \u0026quot;ISO-8859-1\u0026quot;, skiprows = 2, header = None) starwars_cols = pd.read_csv(url, encoding = \u0026quot;ISO-8859-1\u0026quot;, nrows = 2, header = None) starwars_cols.iloc[0,:].str.upper().str.replace(\u0026quot; \u0026quot;, \u0026quot;!\u0026quot;) \nValidating statistical summaries len(), .query(), and .value_counts() will be your friends.\nValidating visuals You\u0026rsquo;re going to make a lot of bar charts!\n Simple bar chart tutorial. Make Altair do the counting for you! Tutorials here and here.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p5\/d2\/"},{value:"Day 2: Transforming Data",label:"<p>Welcome to class! Spiritual Thought Announcements  Code chunk options:  Locally using #| warning: false Globally in the YAML using execute: warning: false    Flights Data Issues: What are some of the data issues you discovered while getting to know your data?\nLoading JSON files into pandas Let\u0026rsquo;s load in some practice data! Data link.\nHere\u0026rsquo;s a description of the data: Data Description.\nimport pandas as pd # to load and transform data import numpy as np # for math\/stat calculations # from url to pandas dataframe url = \u0026#34;https:\/\/github.com\/byuidatascience\/data4missing\/raw\/master\/data-raw\/mtcars_missing\/mtcars_missing.json\u0026#34; cars = pd.read_json(url) # or from file to pandas dataframe cars = pd.read_json(\u0026#34;mtcars_missing.json\u0026#34;) Look at the data for the first two cars. What is different about the format?\n[ { \u0026#34;car\u0026#34;: \u0026#34;Mazda RX4\u0026#34;, \u0026#34;mpg\u0026#34;: 21, \u0026#34;cyl\u0026#34;: 6, \u0026#34;disp\u0026#34;: 160, \u0026#34;hp\u0026#34;: 110, \u0026#34;drat\u0026#34;: 3.9, \u0026#34;wt\u0026#34;: 2.62, \u0026#34;qsec\u0026#34;: 16.46, \u0026#34;vs\u0026#34;: 0, \u0026#34;am\u0026#34;: 1, \u0026#34;gear\u0026#34;: 4, \u0026#34;carb\u0026#34;: 4 }, { \u0026#34;car\u0026#34;: \u0026#34;Mazda RX4 Wag\u0026#34;, \u0026#34;mpg\u0026#34;: 21, \u0026#34;cyl\u0026#34;: 6, \u0026#34;disp\u0026#34;: 160, \u0026#34;hp\u0026#34;: 110, \u0026#34;drat\u0026#34;: 3.9, \u0026#34;wt\u0026#34;: 2.875, \u0026#34;qsec\u0026#34;: 17.02, \u0026#34;am\u0026#34;: 1, \u0026#34;gear\u0026#34;: 4, \u0026#34;carb\u0026#34;: 4 } ] \nYour Turn: Transforming Data With your group, research these functions and create an example using the cars data. Post your example in Slack. Be prepared to teach the class about your functions.\nYou can use the Data Transformation textbook chapter and the pandas documentation to help you.\nRecreate the following output to the best of your abilities: Group 1: Working with rows  .query() allows you to subset observations (rows) .sort_values() arranges rows in a particular order  Group 2: Working with columns  .filter() (as well as [] and .loc[]) allow you to select columns .assign() is one way to add new columns to a dataframe  Group 3: Counting items  .value_counts() summarizes a column by counting the values inside .crosstab() creates a \u0026ldquo;cross tabulation\u0026rdquo; of two or more variables  Group 4: Summarizing data  Using .groupby() and .agg() together allows you to calculate group summaries  Your Turn: Summarizing the cars data Write code to calculate the mean weight wt for each cylinder type cyl.\nAnswer 1    cars.groupby(\u0027cyl\u0027).agg(mean_weight = (\u0027wt\u0027, np.mean)).reset_index()    Can you print the answer as a markdown table?\nAnswer 2    print(cars.groupby(\u0027cyl\u0027).agg(mean_weight = (\u0027wt\u0027, np.mean)).reset_index().to_markdown(index = False))    Project 2 FAQs Why are we using assign()    One main reason:\n You can create multiple columns within the same assign() where one of the columns depends on another one defined within the same assign. source: Documentation\n Other resources:\n Why use pandas.assign rather than simply initialize new column? 3 Ways to Add New Columns to Pandas Dataframe  Not related, but also fun: Should you use \u0026ldquo;dot notation\u0026rdquo; or \u0026ldquo;bracket notation\u0026rdquo; with pandas?\n   Lambda functions    Two ways to define the same function:\ndef square(x): return x**2 square = lambda x:x**2 There are some difference between them as listed below.\n  lambda is a keyword that returns a function object and does not create a \u0026lsquo;name\u0026rsquo;. Whereas def creates name in the local namespace lambda functions are good for situations where you want to minimize lines of code as you can create function in one line of python code. It is not possible using def lambda functions are somewhat less readable for most Python users. lambda functions can only be used once, unless assigned to a variable name.  source\n    Conditional operations    What if you want to create a new column, whose values depend on another column? There are a lot of ways to accomplish this (see this stackoverflow answer). Some functions I use:\n isin() method where() method You can also use an if else statement inside a lambda function     Missing data    We will learn how to identify and deal with missing data next week. For now, we can drop rows we don\u0026rsquo;t want using square brackets [] or .query().   API\u0026rsquo;s and JSON: A Primer Application Programming Interfaces (APIs) Representational State Transfer (REST APIs)  Over the course of the ’00s, another Web services technology, called Representational State Transfer, or REST, began to overtake [all other tools] for the purpose of transferring data. One of the big advantages of programming using REST APIs is that you can use multiple data formats — not just XML, but JSON and HTML as well. As web developers came to prefer JSON over XML, so too did they come to favor REST over SOAP. As Kostyantyn Kharchenko put it on the Svitla blog, “In many ways, the success of REST is due to the JSON format because of its easy use on various platforms.”\nToday, JSON is the de-facto standard for exchanging data between web and mobile clients and back-end services. ref\n JavaScript Object Notation  Well, when you’re writing frontend code in Javascript, getting JSON data back makes it easier to load that data into an object tree and work with it. And JSON formats data in a more succinct way, which saves bandwidth and improves response times when sending messages back and forth to a server. In a world of APIs, cloud computing, and ever-growing data, JSON has a big role to play in greasing the wheels of a modern, open web. ref\n Other Resources  RESTful APIs in 100 Seconds (video) Python API Tutorial: Getting Started with APIs Big List of Free and Open Public APIs (No Auth Needed)  How could we leverage numpy\u0026rsquo;s where() to address the different month proportions in question 3?    reference   How many rows have missing months?    flights.month.value_counts()    Can we figure out any patterns in the missingness?     pd.crosstab() groupby     -------------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p2\/d2\/"},{value:"JSONs \u0026 missing",label:"<p>UFO Sightings Data Link to json file\nExercise 1 Read in the json file as a pandas dataframe. After reading in the data, you\u0026rsquo;ll want to explore it and gain some intuition. Exploring data is a very important step — the more you know about your data the better! Answer the following questions to gain some insight into this dataset.\n How many rows are there? How many columns? What does a row represent in this dataset? What are the different ways missing values are encoded? How many np.nan in each column?  Some useful code for exploring data\n# Object\/Categorical Columns data.column_name.value_counts(dropna=False) data.column_name.unique() # Numeric Columns data.column_name.describe() # Counting missing values data.isna().sum() # Creates boolean dataframe and sums each column  Exercise 2 After learning different ways our data encodes missing values, now we will neatly manage them. There are many techniques we can use to handle missing values; for example, we can drop all rows that contain a missing value, impute with mean or median, or replace missing values with a new missing category. We will use some of these techniques in this exercise.\n shape_reported - replace missing values with missing string. distance_reported - change -999 values to np.nan. (-999 is a typical way of encoding missing values.) distance_reported - fill in missing values with the mean (imputation) were_you_abducted - replace - string with missing string.  The first 10 rows of your data should look like this after completion of the above steps.\n    city shape_reported distance_reported were_you_abducted estimated_size     0 Ithaca TRIANGLE 8521.9 yes 5033.9   1 Willingboro OTHER 7438.64 no 5781.03   2 Holyoke OVAL 7438.64 no 697203   3 Abilene DISK 7438.64 no 5384.61   4 New York Worlds Fair LIGHT 6615.78 missing 3417.58   5 Valley City DISK 7438.64 no 4280.1   6 Crater Lake CIRCLE 7377.89 no 528289   7 Alma DISK 7438.64 missing 4772.75   8 Eklutna CIGAR 5214.95 no 4534.03   9 Hubbard CYLINDER 8220.34 missing 4653.72    Some useful code for filling in missing data\ndata.column_name.replace(..., ..., inplace=True) data.column_name.fillna(..., inplace=True)  Exercise 3 Create a table that contains the following summary statistics.\n median estimated size by shape mean distance reported by shape count of reports belonging to each shape  Your table should look like this:\n   shape_reported median_est_size mean_distance_reported group_count     CIGAR 5899.68 6520.21 3   CIRCLE 266002 7408.26 2   CYLINDER 4550.58 8039.49 2   DISK 4581.8 7516.39 16   FIREBALL 5407.22 7097.78 3   FLASH 6108.34 7438.64 1   FORMATION 5104.4 8708.32 2   LIGHT 3850.25 7636.09 2   OTHER 4699.4 7473.98 4   OVAL 4943.63 7787.24 4   RECTANGLE 3668.1 6054.62 2   SPHERE 5076.78 7206.55 6   TRIANGLE 5033.9 8521.9 1   missing 250153 7438.64 2    Some useful code for grouping and getting summary statistics\n(data.groupby(...) .agg(..., ..., ...))  Exercise 4 The cities listed below reported their estimated size in square inches, not square feet. Create a new column named estimated_size_sqft in the dataframe, that has all the estimated sizes reported as sqft. (Hint: divide by 144 to go from sqin -\u0026gt; sqft)\n Holyoke Crater Lake Los Angeles San Diego Dallas  The head of your data should look like this.\n    city shape_reported distance_reported were_you_abducted estimated_size estimated_size_sqft     0 Ithaca TRIANGLE 8521.9 yes 5033.9 5033.9   1 Willingboro OTHER 7438.64 no 5781.03 5781.03   2 Holyoke OVAL 7438.64 no 697203 4841.69   3 Abilene DISK 7438.64 no 5384.61 5384.61   4 New York Worlds Fair LIGHT 6615.78 missing 3417.58 3417.58   5 Valley City DISK 7438.64 no 4280.1 4280.1   6 Crater Lake CIRCLE 7377.89 no 528289 3668.68   7 Alma DISK 7438.64 missing 4772.75 4772.75   8 Eklutna CIGAR 5214.95 no 4534.03 4534.03   9 Hubbard CYLINDER 8220.34 missing 4653.72 4653.72    Some useful code to fix the rows reported in sqin\nnp.where(..., # Condition ..., # If condition is true ...) # If condition is false  After you have completed this skill builder with your team (or on your own) then compare your work to our script    See the script.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/json_missing\/"},{value:"Project 2: Late flights and missing data (JSON files)",label:"<p>Background Delayed flights are not something most people look forward to. In the best case scenario you may only wait a few extra minutes for the plane to be cleaned. However, those few minutes can stretch into hours if a mechanical issue is discovered or a storm develops. Arriving hours late may result in you missing a connecting flight, job interview, or your best friend’s wedding.\nIn 2003 the Bureau of Transportation Statistics (BTS) began collecting data on the causes of delayed flights. The categories they use are Air Carrier, National Aviation System, Weather, Late-Arriving Aircraft, and Security. You can visit the BTS website to read definitions of these categories.\nThe JSON file for this project contains information on delays at 7 airports over 10 years. Your task is to clean the data, search for insights about flight delays, and communicate your results using the provided template. If you have completed the checkpoints for Unit 5, then you are ready to answer the Grand Questions listed below. Refer to the readings for additional help.\nData Download: JSON File\nInformation: Data Description\nReadings  P4DS: Section 12.1 \u0026amp; 12.2 Tidy data P4DS: Chapter 5 Data transformation P4DS: Section 7.4 Missing Values Python Data Science Handbook: Missing Data Wikipedia Missing Data  Optional References  isin method where method np.where method replace method An introduction to JSON (May need to open in ingognito to read.) The key word in \u0026lsquo;Data Science\u0026rsquo; is not Data\u0026hellip; How to Handle Missing Data (May need to open in ingognito to read.)  Questions and Tasks   Which airport has the worst delays? Discuss the metric you chose, and why you chose it to determine the “worst” airport. Your answer should include a summary table that lists (for each airport) the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours.\n  What is the best month to fly if you want to avoid delays of any length? Discuss the metric you chose and why you chose it to calculate your answer. Include one chart to help support your answer, with the x-axis ordered by month. (To answer this question, you will need to remove any rows that are missing the Month variable.)\n  According to the BTS website, the “Weather” category only accounts for severe weather delays. Mild weather delays are not counted in the “Weather” category, but are actually included in both the “NAS” and “Late-Arriving Aircraft” categories. Your job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild). You will need to replace all the missing values in the Late Aircraft variable with the mean. Show your work by printing the first 5 rows of data in a table. Use these three rules for your calculations:__\n 100% of delayed flights in the Weather category are due to weather  30% of all delayed flights in the Late-Arriving category are due to weather. From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.    Using the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.\n  Fix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”). In your report include one record example (one row) from your new data, in the raw JSON format. Your example should display the \u0026ldquo;NaN\u0026rdquo; for at least one missing value.__\n  Deliverables Use this template to submit your Client Report. The template has three sections (for additional details please see the instructional template):\n A short summary that highlights key that describes the results describing insights from metrics of the project and the tools you used (Think “elevator pitch”). Answers to the grand questions. Each answer should include a written description of your results, code snippets, charts, and tables.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/project-2\/"},{value:"Day 1: Exploring names with pandas",label:"<p>Welcome to Class! Announcements The data science lab\nCompleting Last Week  Quarto - \u0026ldquo;out of the frying pan and into the fire\u0026rdquo; Finishing the Introduction Project  Use the QMD Template project template Render as HTML and upload in Canvas    What was that data science community portion of our grade?    The Syllabus has this section which says:\n Data science community\n To earn credit for the DS Community element you must complete two different tasks from the list below. At the end of the semester, you will be asked to report on which tasks you completed and what you learned from them.\nAttend Data Science Society at least once.\n Sign up for an email newsletter that will teach you more about data science. Data Science Weekly or Data Elixir are good options. Listen to a podcast episode about data science. Build a Career in Data Science has some excellent episodes. Watch a professional presentation on YouTube about data science. Be prepared to share the link and a summary of the video. Reach out to someone who works in a data-related field and ask them for 15 minutes of their time. Use this time to conduct an “informational interview” and learn more about their responsibilities and career path. Research and apply to at least 5 data-related jobs or internships.  Interview Question: How do you keep up with the current methods in data science?\nDon\u0026rsquo;t Say: Nothing\n   Let\u0026rsquo;s Code! DS 250 workflow     You are going to hit SHIFT \u002b ENTER thousands of times. We don\u0026rsquo;t usually source our scripts. Think of Python Interactive like a graphing calculator or Excel on steroids. You code in pieces. Rewrite for clarity!     Can you figure out the functions of pandas?    Pandas Cheat Sheet and Basics Blog Post\n # Pause: can you explain what this code is doing? df = pd.DataFrame( {\u0026#34;a\u0026#34; : [5, 4, 6, 2, 3], \u0026#34;b\u0026#34; : [7, 8, 9, 10, 11], \u0026#34;c\u0026#34; : [10, 11, 12, 101, 0]}) Use the cheat sheet to find the functions you would need to implement the following steps.\nGroup 1\n sort my table by column a then only use the first 2 rows then calculate the mean of column b.  Group 2\n rename column a to duck then subset to only have duck and b columns then keep all rows where b is less than 9 then find the min of duck     What is method chaining?    Pandas and Altiar are built to allow for method chaining. Here is a great resource on how to use method chaining: How to write neat pandas code.\n Altair creates a chart object pandas creates a DataFrame object We usually include () around our entire method so we can show it in steps.     Project 1 - Intro Understanding your data You should be able to introduce your data sets to people, the same way you introduce a friend!\n What does each row represent? If you don\u0026rsquo;t know, then you don\u0026rsquo;t understand what groups you can analyze. What does each column represent? If you don\u0026rsquo;t know, then you don\u0026rsquo;t understand what information you can evaluate for each group.  Being able to explain your data out loud to someone else follows the same principles as rubber duck debugging.\nIntroduction to pandas \u0026ldquo;DataFrame\u0026rdquo; What is a pandas DataFrame? We can read the official documentation. I also like the video in this tutorial.\nDataFrames come with attributes and built-in functions that can help us get a feel for our data.\nRun the code below one line at a time (or use other functions of your choice) to explore the names data. What do you learn?\nmy_data.columns my_data.shape my_data.size my_data.head() my_data.describe() Setup for Project 1 Create the folder and files to get prepared.  DS250 \u0026gt; project_1 \u0026gt;  names.py names.qmd data.csv (just in case the internet is down)    \u0026ldquo;How should we start each file?\u0026rdquo; I would do this process for every project.\n names.py: Every file starts with the same cells 1) import packages, 2) load data. names.md: Let\u0026rsquo;s start with the course template notes.md: Keep project noteson the readings and things you learn. my_cheat_sheet.md: Update your own cheat sheet  Read in the data.\n#%% # load packages import pandas as pd import altair as alt #%% # load data url = \u0026#34;https:\/\/github.com\/byuidatascience\/data4names\/raw\/master\/data-raw\/names_year\/names_year.csv\u0026#34; names = pd.read_csv(url) 1. How many unique names does the names dataframe contain? Work with a partner to find the answer. You might want to look at this pandas cheat sheet.\nHint     Pull the name column out as a series Use the pandas unique function pd.unique() Find the size of the series     2. What is the range of years in the names dataframe? Again, work with a partner and use the pandas cheat sheet.\nHint2     Pull the year column out as a series Find the max Find the min     </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p1\/d1\/"},{value:"Day 1: Intro to Flights Data",label:"<p>Welcome to class! Spiritual Thought Short Link\nProject 1 Comments  Don\u0026rsquo;t include data as a table. Only include tables that add useful information. If I have to scroll up and down it isn\u0026rsquo;t useful. Reports should be readable by an intelligent, but non-technical audience (Meaningful titles and section names) Make it like something you\u0026rsquo;d like to read Clean out any code output, logs, that distract from the message (\u0026ldquo;My Useless Chart\u0026rdquo;) Eliminate \u0026ldquo;warnings\u0026rdquo;  Project 2: Late Flights and Missing Data JSON files (JavaScript Object Notation)  Today, JSON is the de-facto standard for exchanging data between web and mobile clients and back-end services. source\n What is JSON? Introduce the data Load the JSON file and spend a few minutes studying it. Can you learn enough about it to describe the columns and rows?\nHints:\n You can use .describe() to learn about the distribution of a numeric variable. You can use .value_counts() to learn about the distribution of a categorical variable. .crosstab() creates a \u0026ldquo;cross tabulation\u0026rdquo; of two or more categorical variables.  Can you trust the data? Do you notice anything interesting about the flights data?\nQuestion Brainstorming In your group, try to answer the following questions about your assigned \u0026ldquo;Grand Question\u0026rdquo;:\n What is our goal? How can we get there? What will the answer look like when we\u0026rsquo;re done?  Project 2 FAQs Missing data    Not all missing data is represented as np.nan. For an example, look at the column that counts delays due to late aircraft.\nWe will learn how to identify and deal with missing data next week. For now, we can drop rows we don\u0026rsquo;t want using square brackets [] or .query().\n   What columns do we need to use for question 3 (total number of flights delayed by weather)?      num_of_delays_weather num_of_delays_late_aircraft num_of_delays_nas      Groups 1 and 5 - Working with rows  .query() allows you to subset observations (rows) .sort_values() arranges rows in a particular order  Groups 2 and 6 - Working with columns  .filter() (as well as [] and .loc[]) allow you to select columns .assign() is one way to add new columns to a dataframe  Groups 3 and 7 - Counting items  .value_counts() summarizes a column by counting the values inside .crosstab() creates a \u0026ldquo;cross tabulation\u0026rdquo; of two or more variables  Groups 4 and 8 - Summarizing data  Using .groupby() and .agg() together allows you to calculate group summaries   Your Turn: Summarizing the cars data Write the code to calculate the mean weight wt for each cylinder type cyl.\nAnswer 1    cars.groupby(\u0027cyl\u0027).agg(mean_weight = (\u0027wt\u0027, np.mean)).reset_index()    Can you print the answer as a markdown table?\nAnswer 2    cars.groupby(\u0027cyl\u0027).agg(mean_weight = (\u0027wt\u0027, np.mean)).reset_index().to_markdown(index = False)    -------------------------------------------------------------------------- The flights data How are we going to answer Question 1 and Question 2?\nWatch out for different forms of missing data!    Not all missing data is represented as np.nan. For an example, look at the column that counts delays due to late aircraft.   What columns do we need to use for question 3 (total number of flights delayed by weather)?      num_of_delays_weather num_of_delays_late_aircraft num_of_delays_nas      How could we leverage numpy\u0026rsquo;s where() to address the different month proportions in question 3?    reference   How many rows have missing months?    flights.month.value_counts()    Can we figure out any patterns in the missingness?     pd.crosstab() groupby     Project 1: Names In your groups, discuss:\n What did you learn about data and Altair? What questions do you still have?  Connecting to Application Programming Interfaces (APIs) Representational State Transfer (REST APIs)  Over the course of the ’00s, another Web services technology, called Representational State Transfer, or REST, began to overtake [all other tools] for the purpose of transferring data. One of the big advantages of programming using REST APIs is that you can use multiple data formats — not just XML, but JSON and HTML as well. As web developers came to prefer JSON over XML, so too did they come to favor REST over SOAP. As Kostyantyn Kharchenko put it on the Svitla blog, “In many ways, the success of REST is due to the JSON format because of its easy use on various platforms.”\nToday, JSON is the de-facto standard for exchanging data between web and mobile clients and back-end services. ref\n JavaScript Object Notation  Well, when you’re writing frontend code in Javascript, getting JSON data back makes it easier to load that data into an object tree and work with it. And JSON formats data in a more succinct way, which saves bandwidth and improves response times when sending messages back and forth to a server. In a world of APIs, cloud computing, and ever-growing data, JSON has a big role to play in greasing the wheels of a modern, open web. ref \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026gt;\n What does missing data look like?    How many missing values do you see in the first ten rows? (The mtcars documentation might help.)\ncars.head(10)    How many missing values are there?    #%% cars.isna().sum() #%% cars.isin([\u0026#39;\u0026#39;]).sum() #%% cars.describe() reference 1 and reference 2\n   ### How Pandas handles missingness Read [\u0027Handling missing in pandas\u0027](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/missing_data.html#calculations-with-missing-data) ```python import numpy as np df = (pd.DataFrame( np.random.randn(5, 3), index=[\u0027a\u0027, \u0027c\u0027, \u0027e\u0027, \u0027f\u0027, \u0027h\u0027], columns=[\u0027one\u0027, \u0027two\u0027, \u0027three\u0027]) .assign( four = \u0027bar\u0027, five = lambda x: x.one  0, six = [np.nan, np.nan, 2, 2, 1], seven = [4, 5, 5, np.nan, np.nan]) ) ``` What happens when you add two pandas objects with missing values?    df.seven \u002b df.six reference\n   What happens when you sum within a column?    df.seven.sum() reference\n   How could I add two columns treating NaN like zeros?    df.seven.fillna(0) \u002b df.six.fillna(0) reference\n   ----------------------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p2\/d1\/"},{value:"Day 1: Intro to ML",label:"<p>Welcome to class! Announcements  Project 3 - Getting pickier about good communication  Career batting average Meaningful report name (Drop \u0026ldquo;Client Report\u0026rdquo;) Meaningful section headers so the table of contents is useful (don\u0026rsquo;t call them \u0026ldquo;Grand Questions\u0026rdquo;) Don\u0026rsquo;t include \u0026ldquo;My useless chart\u0026rdquo; from the template   Coding Challenge - Table Ask for help!  Computing lab Computing lab Slack channel (search) Slack classmates or general channel    Spiritual Thought What did you learn about the Sermon on the Mount? What does it mean to be the light of the world? Pictionary!   ----------------------  From Sebastian Thrun:\n AI is able to learn \u0026lsquo;rules\u0026rsquo; from highly repetitive data.\nThe single most important thing for AI to accomplish in the next ten years is to free us from the burden of repetitive work.\n Your Turn: Student Classification Problem Can we predict if a student is from Utah?\nYour Turn: Features and Targets Import dwellings.csv. With a neighbor:\n Try to describe the data. Explain what each observation (row) is and what measurements we have on that observation (columns). Now try describing the modeling (machine learning) we are going to do in terms of \u0026ldquo;features\u0026rdquo; and \u0026ldquo;targets\u0026rdquo;. Watch out - are there any columns that are the target in disguise? (You may need to review the project goal.) What features do you expect to have a strong relationship with the target?  Before Next Class Do the project readings    Machine Learning Introduction\n Step-by-step guide (mostly) for training a GaussianNB classifier. (The steps will be the same for any algorithm you use.)  Visual Introduction to Machine Learning\n Machine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. You can use it to make predictions. One method for making predictions is called a decision trees, which uses a series of if-then statements to identify boundaries and define patterns in the data. Overfitting happens when some boundaries are based on distinctions that don\u0026rsquo;t make a difference. You can see if a model overfits by having test data flow through the model.     Start working on Question 1    The goal of Grand Question 1 is to help us with \u0026ldquo;feature selection\u0026rdquo;.\n Remember: Overfitting happens when some boundaries are based on on distinctions that don\u0026rsquo;t make a difference. More data does not always lead to better models. (Occam\u0026rsquo;s Razor)  Common questions:\n Why it may be better to have fewer predictors in Machine Learning models? What is Feature Selection and why do we need it in Machine Learning?     What is the 5000 rows error with Altair?    MaxRowsError: How can I plot Large Datasets?\nYou may also save data to a local filesystem and reference the data by file path. Altair has a JSON data transformer that will do this transparently when enabled:\nalt.data_transformers.disable_max_rows() subset_data = denver.sample(n = 4999)    scikit-learn resources     Home page Tutorials Getting Started: What do you notice about the header portion of each of the script chunks?  import vs from ... import       1. Models approximate real-life situations using limited data.  2. In doing so, errors can arise due to assumptions that are overly simple (bias) or overly complex (variance).  3. Building models is about making sure there\u0027s a balance between the two. #### But what is the \u0027Pavlovian bell\u0027 in the machine learning model? ![](..\/..\/images\/ml\/test.png) Some mathematical penalty\/reward equation.  - __[Regression](https:\/\/setosa.io\/ev\/ordinary-least-squares-regression\/)__  - __[Variance, RMSE, SD](..\/..\/interactive\/threshold_histogram.html)__  - __proportions__ ## Using our project data to understand features, targets, and samples.  1. Import `dwellings_ml.csv` and write a short sentence describing your data. Remember to explain an observation and what measurements we have on that observation.  2. Now try describing the modeling (machine learning) we are going to do in terms of features and targets.  A. Are there any columns that are the target in disguise?  B. _Are the observational units unique in every row?_ ![](..\/..\/images\/ml\/iris_description.png) --------------- - Financial: orders, invoices, payments  - Work: plans, activity records  - School: Grades ------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p4\/d1\/"},{value:"Day 1: Intro to Project 3",label:"<p>Welcome to class! Spiritual Thought Announcements  Project 2 Highlights Project 2 comments   Turn them in Clean up graphs (main titles, axis labels, legends) Column headers on tables in your report (don\u0026rsquo;t include index number either) Technically Proportion of all flights delayed by weather, not the proportion of delayed flights JSON should look like a text example of a record, not a table  Things for next project:   Be sure to give section headers meaningful titles (NOT \u0026ldquo;Grand Question 1\u0026rdquo;)  What is Structured Query Language (SQL)?   Ray and I were impressed by how compactly Codd’s languages could represent complex queries. However, at the same time, we believed that it should be possible to design a relational language that would be more accessible to users without formal training in mathematics or computer programming. We believed that barriers to widespread acceptance of Codd’s languages existed on two levels.  .   1. The first barrier came from the mathematical notation, which was hard to enter at a keyboard. This barrier was superficial and could be easily dealt with by replacing symbols with keywords.   2. The more difficult barrier was at the semantic level. The basic concepts of Codd’s languages were adapted from set theory and symbolic logic. This was natural given Codd’s background as a mathematician, _but Ray and I hoped to design a relational language based on concepts that would be familiar to a wider population of users._ We also hoped to extend the language to encompass database updates and administrative tasks such as the creation of new tables and views, which had traditionally been outside the scope of a query language. SQL is \u0022a relational language based on concepts that would be familiar to a wider population of users.\u0022  When we moved to the San Jose Research Laboratory in 1973 to join the System R project, we began work on another new language that we called Sequel. Sequel allowed the well-paid-employee query to be represented in a readable form free from mathematical concepts and symbols. ... In 1977, because of a trademark issue, the name Sequel was shortened to SQL.  ------------------------------------- Ok, but how does it work? SQL uses keywords to pull (or \u0026ldquo;fetch\u0026rdquo;, \u0026ldquo;extract\u0026rdquo;) the data we want from a database. The computer reads those keywords in a specific order.\nFrom EverSQL we can get some more background:\n This is the logical order of operations, also known as the order of execution, for an SQL query:\n  FROM, including JOINs WHERE GROUP BY HAVING WINDOW functions SELECT DISTINCT UNION ORDER BY LIMIT and OFFSET   But the reality isn\u0026rsquo;t that easy nor straight forward. As we said, the SQL standard defines the order of execution for the different SQL query clauses. Said that, modern databases are already challenging that default order by applying some optimization tricks which might change the actual order of execution, though they must end up returning the same result as if they were running the query at the default execution order.\n For CSE 250: Don\u0026rsquo;t think too hard about optimization at this point. Let the database figure out the optimized routine.\nMost SQL queries are typed in the following pattern:\nSELECT -- \u0026lt;columns\u0026gt; and \u0026lt;column calculations\u0026gt; FROM -- \u0026lt;table name\u0026gt;  JOIN -- \u0026lt;table name\u0026gt;  ON -- \u0026lt;columns to join\u0026gt; WHERE -- \u0026lt;filter condition\u0026gt; GROUP BY -- \u0026lt;subsets for column calculations\u0026gt; HAVING -- \u0026lt;grouped filter condition\u0026gt; ORDER BY -- \u0026lt;how the output is returned in sequence\u0026gt; LIMIT -- \u0026lt;number of rows to return\u0026gt; \nProject 3 - what are our goals? Do we understand the questions being asked in Project 3?\nThe baseball data Let\u0026rsquo;s start exploring the baseball data!\n You\u0026rsquo;ll need to download the SQLite Databse And review the data dictionary  import pandas as pd import sqlite3 con = sqlite3.connect(\u0026#39;lahmansbaseballdb.sqlite\u0026#39;) df = pd.read_sql_query(\u0026#34;SELECT * FROM fielding LIMIT 5\u0026#34;, con) df How can we see what tables are in the database?\nimport pandas as pd import sqlite3 con = sqlite3.connect(\u0026#39;lahmansbaseballdb.sqlite\u0026#39;) pd.read_sql_query(\u0026#34;\u0026#34;\u0026#34; SELECT name FROM sqlite_master WHERE type=\u0026#39;table\u0026#39; \u0026#34;\u0026#34;\u0026#34;, con) Understanding SQL queries Make sure you do the project readings!\nWhat table do we want to use?    q = \u0026#39;\u0026#39;\u0026#39; SELECT * FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What columns do we want to select?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What calculation do we want to perform?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r, ab\/r FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; batting_calc = dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    What name do we give our calculated column?    q = \u0026#39;\u0026#39;\u0026#39; SELECT playerid, teamid, ab, r, ab\/r as runs_atbat FROM batting LIMIT 5 \u0026#39;\u0026#39;\u0026#39; batting_calc = dw.query(\u0026#39;byuidss\/cse-250-baseball-database\u0026#39;, q).dataframe    -------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p3\/d1\/"},{value:"Day 1: The war with Star Wars",label:"<p>Welcome to class! Spiritual Thought Announcements  Project 4 thoughts  Feature Importances - Sorted Bar Graph, not unsorted tables Suppress warnings And the winner is\u0026hellip;    The Star Wars data Load the Star Wars data # %% import pandas as pd import altair as alt import numpy as np url = \u0026#39;https:\/\/github.com\/fivethirtyeight\/data\/raw\/master\/star-wars-survey\/StarWars.csv\u0026#39; dat = pd.read_csv(url) \n??? What do the data look like? Take the time to understand how the current data is organized.\nFirst things first\u0026hellip; Each group should answer these questions:\n Where are the column names? What does each row represent? What does each column represent?  What do we want the data to look like? Each group should answer these questions:\n What is the goal of this project, and how does that affect what we want from the data? What do we want each row to represent? What do we want each column to look like? Pick a few columns from the dataset and try creating an example in excel.  Cleaning data takes time Maybe not 80% of your time, but it does take time!\n Data science is frequently about doing bespoke analysis which means creating and labelling unique datasets. No matter how cleanly formatted or standardized a dataset is, it likely needs some work.\nI would argue that spending time working with data to transform, explore and understand it better is absolutely what data scientists should be doing. This is the medium they are working in. Understand the material better and you\u0026rsquo;ll get better insights. ref\n Structure your project, structure your thinking Tableau on tidying data  Think about your data holistically Know the basic structure of your data Keep track of your steps Spot check throughout  Compartmentalize and organize your scripts and data  Best practices for organizing data science projects How to organize your Python data science project Cookiecutter Data Science Data Science Project Folder Structure  - [BYU=I DSS](https:\/\/github.com\/BYUIDSS\/blank_project_repository) ----- What are codecs and encodings?  UTF-8 Python Unicode Basics pd.read_csv() ISO-8859-1  The .str functions in pandas  .strip: Strip white space .replace: replace one string of characters with another. .split: Separate a character string into two values. .join: Join two lists together Python for Data Science: Strings Pandas Documentation  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p5\/d1\/"},{value:"Machine Learning",label:"<p>Introduction Everyone seems to have a slightly different take on the differences between Artificial Intelligence, Machine Learning, and Data Science. The following four articles cover some of the most common definitions.\nAs you read them, think about the differences and similarities of the definitions. Given the backgrounds of the various authors, whose opinions might you give more weight to?\n Michael Copeland writing for NVidia Bernard Marr writing for Forbes Vincent Granville writing for Data Science Central Simply Statistics Blog - The key word in \u0026ldquo;Data Science\u0026rdquo; is not Data, it is Science  Of particular note is this quote from the Granville article:\n Earlier in my career (circa 1990) I worked on image remote sensing technology, among other things to identify patterns (or shapes or features, for instance lakes) in satellite images and to perform image segmentation: at that time my research was labeled as computational statistics, but the people doing the exact same thing in the computer science department next door in my home university, called their research artificial intelligence. Today, it would be called data science or artificial intelligence, the sub-domains being signal processing, computer vision or IoT.\n As with most things in the realm of science, there tends to be a wide gap between how the media, government, and business sectors view a particular technology compared to how it\u0026rsquo;s viewed by the engineers and scientists using that technology.\nFor our purposes in this course, we\u0026rsquo;ll define these terms as follows:\n Artificial Intelligence: The study of man-made \u0026ldquo;agents\u0026rdquo; that perceive their environment and take actions that maximize their chances of success at some goal.1\nMachine Learning: A subfield within Artificial Intelligence that gives \u0026ldquo;computers the ability to learn without being explicitly programmed.\u0026quot;2\nData Science: The study and use of the techniques, statistics, algorithms, and tools needed to extract knowledge and insights from data.3\n MORAVEC\u0026rsquo;S PARADOX In the 1980\u0026rsquo;s, Hans Moravec made the following observation, which came to be known as Moravec\u0026rsquo;s Paradox:\n \u0026hellip;as the number of demonstrations has mounted, it has become clear that it is comparatively easy to make computers exhibit adult-level performance in solving problems on intelligence tests or playing checkers, and difficult or impossible to give them the skills of a one-year-old when it comes to perception and mobility.4\n So, while AI and machine learning algorithms can accomplish many tasks much better than humans can, any toddler can outperform even the most state-of-the-art neural network in picking out photos of their parents or pet cat.\n5\nEven though Moravec wrote about this over thirty years ago, the same sentiment persists in AI research today. In a 2016 interview, Dr. Sean Holden an AI researcher at Cambridge University, discussed the differences between human intelligence and artificial intelligence:\n “Most AI researchers don’t try to solve the whole problem because it’s too hard. They take some specific problem and do it better. That’s not to say that the way humans think isn’t useful to AI, but working out how brains do things is hard. And there’s a difference in scale. Brains are doing things that are in some senses quite different from what AI researchers are currently attacking – I’d be ecstatic, for example, if I could build a robot that could put on a duvet cover.”6\n Dr. Fumiya Iida, from the Machine Intelligence Lab at Cambridge, adds:\n “We have hundreds of thousands of muscles in our body, so how can the brain control this? A computer can’t. Every fraction of a second you have to co-ordinate hundreds of muscles just to grab a cup, for example.”6\n PREDICTION VS. INFERENCE In machine learning, we are typically interested in doing one of two things: making inferences, or making predictions.\n Inference: Given a set of data you want to infer how the output is generated as a function of the data.\nPrediction: Given a new measurement, you want to use an existing data set to build a model that reliably chooses the correct identifier from a set of outcomes.7\n This example explains the differences between those two goals:\n Inference: You want to find out what the effect of Age, Passenger Class and, Gender has on surviving the Titanic Disaster. You can put up a logistic regression and infer the effect each passenger characteristic has on survival rates.\nPrediction: Given some information on a Titanic passenger, you want to choose from the set {lives,dies} and be correct as often as possible.7\n Classification Algorithms Imagine that you\u0026rsquo;re a big fan of comic books. Over the years, you\u0026rsquo;ve read enough Marvel and DC comics that if I asked you to \u0026ldquo;classify\u0026rdquo; which universe Superman belonged to, you\u0026rsquo;d be able to confidently say, \u0026ldquo;The DC Universe\u0026rdquo;.\nOr, let\u0026rsquo;s say you\u0026rsquo;ve eaten a lot of chocolate in your life. If I were to have you close your eyes and take a bite of chocolate, you might be able to accurately tell me if it was white chocolate, milk chocolate, semi-sweet, or dark.\nThese are both classification problems. Based on your prior knowledge or training regarding different groups, you can take an item and sort it into the correct group.\nIn machine learning, classification algorithms, (or classifiers), need to be trained before they can classify things on their own. We can train an algorithm by providing it with lots of examples from each group and telling it which attributes of those samples are important. The more examples we use to train our algorithm, the more accurate the classification of new items will be.\nIn the example below, we’re telling the algorithm “this is what a blue circle looks like\u0026rdquo;, or \u0026ldquo;this is what a green circle looks like\u0026rdquo;, etc\u0026hellip;\nOnce an algorithm has been trained, we can see how well it performs by providing it with test data consisting of new items it hasn\u0026rsquo;t seen yet, and checking to see if it can correctly predict which group the new items belong to.\nThe Iris Dataset ABOUT THE DATA For this example, we will use Fisher\u0026rsquo;s Iris Data.\nThe Iris dataset contains the length and width of the sepals and petals from 150 iris flowers across three different species of iris: Iris setosa, Iris versicolor, and Iris virginica.\nEach row in the Iris dataset represents the measurements of a single flower. We refer to each of these as a sample, observation, or instance.\nEach column in the Iris dataset represents a particular thing being measured about each flower. From left to right we have (in centimeters) the sepal length, the sepal width, the petal length, and the petal width. Each of these is referred to as a feature, attribute, measurement, or dimension.\nThe final column in the dataset is the species of the flower. This final column is often referred to as the target or class of the sample.\nClassifiers Classifier algorithms generally follow the same set of steps. Our goal is to create a classifier that can be provided with the measurements of petals and sepals, and then use that information to predict the species of iris flower we\u0026rsquo;re measuring.\nLoad data The first thing we need to do is load our data. In most cases, there is some pre-processing that has to be done on the data in order to get it to the point where we can start working with it. Often you will need to normalize and encode variables.\n Normalization reading Encoding reading  In this case however, the data is provided to you in the exact format you need:\n sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa .. ... ... ... ... ... 145 6.7 3.0 5.2 2.3 Iris-virginica 146 6.3 2.5 5.0 1.9 Iris-virginica 147 6.5 3.0 5.2 2.0 Iris-virginica 148 6.2 3.4 5.4 2.3 Iris-virginica 149 5.9 3.0 5.1 1.8 Iris-virginica The csv file for the iris data can be found here. There are many ways to load data from a csv file, but one handy way is to use the read_csv function from the Pandas library:\nimport pandas as pd url = \u0026quot;https:\/\/byuistats.github.io\/DS250-Course\/skill_builders\/ml_sklearn\/machine_learning.csv\u0026quot; data = pd.read_csv(url) Split data Next, we\u0026rsquo;ll randomly divide all of the samples into two groups. The first group will consist of our training data, or the samples we\u0026rsquo;ll use to train our classifier. The second group will consist of our test data, the data we\u0026rsquo;ll use to test our classifier.\nThere are many ways to do this, but if have our features (sepal and petal measurements) and targets (species names) in separate arrays, we can use the train_test_split function of the sklearn library to do this for us:\nNote, that if you use pandas to load the csv file, you\u0026rsquo;ll have the data in a single pandas Data Frame. At some point you\u0026rsquo;ll need to split that data frame into two numpy arrays, one containing the features, and the other containing the targets.\nTake a look at the Indexing and Selecting Data page in the Pandas user guide for more details and splitting the data, and the to_numpy function for converting to a numpy array.\nNotice the transformation can be completed before the data is divided into test and training sets. Two numpy arrays can be passed to the train_test_split function to get two sets of arrays back. Alternatively, the data frame can be passed to the test_train_split, and then the test and training data is split into their feature and target components.\nThe following examples assume you\u0026rsquo;ve split the data into features and targets before passing it to test_train_split.\nfrom sklearn.model_selection import train_test_split # features = ... select the feature columns from the data frame # targets = ... select the target column from the data frame # Randomize and split the samples into two groups. # 30% of the samples will be used for testing. # The other 70% will be used for training. train_data, test_data, train_targets, test_targets = train_test_split(features, targets, test_size=.3) You could also use python\u0026rsquo;s built in libraries to randomly shuffle the data, and then use array slicing to split the data into test and training subsets. However if you do, make sure you do it in such a way that you still know which species goes with each set of measurements.\nTrain classifier By providing the algorithm with training data, we allow it to create relationships between the features of a sample and its class. In the case of the Iris data set, we\u0026rsquo;re training our algorithm on how a given set of sepal and petal measurements correlate to the flower\u0026rsquo;s species.\nsklearn has a classifier called GaussianNB which we can use to demonstrate this. GaussianNB is a \u0026ldquo;Naïve Bayes\u0026rdquo; classifier that assumes two things about our data:\n  That the underlying features follow a continuous, normal distribution. (The Gaussian part) That each feature is statistically independent of every other feature. (The Naïve part)   Do you think both of these assumptions are true for the Iris data?\nTo train our classifier, first we create an instance of it, then we use the fit method to teach it about our data:\nfrom sklearn.naive_bayes import GaussianNB classifier = GaussianNB() classifier.fit(train_data, train_targets) Test classifier Now that our classifier has been trained on how to classify iris flowers, it\u0026rsquo;s time to test it to see if it can correctly predict the species of flower from a set of measurements.\nNote that it\u0026rsquo;s very important when testing our algorithm that we only test it on data that was not used to train it. Otherwise, we\u0026rsquo;re only testing it\u0026rsquo;s ability to remember training data. This is why we split the data into two groups.\nTo test our classifier, we\u0026rsquo;ll use the predict method and provide it with our test data. This method will return a list of predicted targets, one for each sample in the test data.\nIn our case, we\u0026rsquo;ll give it a list of petal and sepal measurements it has never seen before, and it will return a list of species predictions, on prediction for each sample in our test data:\ntargets_predicted = classifier.predict(test_data) Assess classifier performance Since we already know which type of iris each sample in the test data corresponds to, we can compare the predictions made by the classifier to the sample\u0026rsquo;s actual species and calculate how well our algorithm performs.\nIf m is the number of correct predictions made, and n is the total number of samples in our test data, then accuracy can be calculated as:\naccuracy = m\/n\nSo if our test data has 20 samples and the classifier predicts the correct flower species for 15 of them, then we would say our algorithm has an accuracy of 75%.\n(Note that accuracy isn\u0026rsquo;t the best metric to use for evaluating classification algorithms. We\u0026rsquo;ll be looking at a few alternatives in the future.)\nSummary To summarize: we take our dataset and divide it in two parts: training data and test data. We use the training data to train the classifier to make classifications, then we use the test data to test how well our classifier performs.\nIf we have a classifier that performs well, we can use it with new data, samples whose groups we don\u0026rsquo;t know ahead of time, and the accuracy metric will give us some idea of how reliable those predictions are.\nIf our classifier performs poorly, we either need to provide it with more training data, modify or replace it, or select a different set of attributes to use as features.\nCSE 450: Machine Learning \u0026amp; Data Mining is the class were you can build depth in Machine Learning and it\u0026rsquo;s applications.\nREFERENCES   Artificial Intelligence: A Modern Approach by Russell and Norvig (Prentice Hall, 2009).↩ \u0026#x21a9;\u0026#xfe0e;\n Some Studies in Machine Learning Using the Game of Checkers, by Arthur L. Samuel (IBM Journal, Vol 3, No 3, 1959).↩ \u0026#x21a9;\u0026#xfe0e;\n Wikipedia article on Data Science.↩ \u0026#x21a9;\u0026#xfe0e;\n Mind Children, by Hans Moravec (Harvard University Press, 1988).↩ \u0026#x21a9;\u0026#xfe0e;\n XKCD 1425: Tasks.↩ \u0026#x21a9;\u0026#xfe0e;\n Cambridge Alumni Magazine, Issue 79, pg 19.↩ \u0026#x21a9;\u0026#xfe0e;\n Cross Validated: Prediction vs Inference.↩ \u0026#x21a9;\u0026#xfe0e;\n   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/machine-learning\/"},{value:"Project 3: Finding relationships in baseball.",label:"<p>Background When you hear the word “relationship” what is the first thing that comes to mind? Probably not baseball. But a relationship is simply a way to describe how two or more objects are connected. There are many relationships in baseball such as those between teams and managers, players and salaries, even stadiums and concession prices. The graphs on Data Visualizations from Best Tickets show many other relationships that exist in baseball.\nFor this project, your client would like developed SQL queries that they can use to retrieve data for use on their website without needing Python. They would also like to see example Altair charts.\nData Data Conection: lahmansbaseballdb\nConnection Instructions: See SQL for Data Science\nReadings  SQL for Data Science Readings (read all links)  Optional References  Why SQL is beating NoSQL, and what this means for the future of data Lahman Data Dictionary  Questions and Tasks   Write an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID\/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\n  This three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n Write an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report. Use the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results. Now calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.    Pick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph in Altair to visualize the comparison. What do you learn?\n  Deliverables Use this template to submit your Client Report. The template has three sections (for additional details please see the instructional template):\n A short summary that highlights key that describes the results describing insights from metrics of the project and the tools you used (Think “elevator pitch”). Answers to the grand questions. Each answer should include a written description of your results, code snippets, charts, and tables.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/project-3\/"},{value:"SQL \u0026 databases",label:"<p>Skill builder (relational database) For this skill builder, we are exploring some important topics in relational databases. This exercise will require you to create SQL queries through python. You may want to at least scan the readings before beginning this task since this serves as an assessment of your understanding of the assigned readings.\nA competent student should be able to finish the exercises within 75 minutes.\nBefore you start Make sure you have installed VS-code, pandas, and Altair on your computer.\nAlso make sure you have gone through the tutorial on under course materials called SQL for Data Science: we assume that you have a connection to your data.\nExercise 1 Readme file A database can consist of more than one table\/data set. A relational database consists of tables\/data sets that share columns. These shared columns then establish the relationship between the tables, thus the name relational database. The relations are sometimes not easily found and they require careful investigations.\nTo understand what is in a relational database, we can start with understanding the tables and the columns within.\nHere is a link to the readme file of the baseball database.\n What is the name of the table that records data about pitchers in the regular seasons?\n  What do the HR and HBP columns mean in that table respectively?\n Excercise 2 SELECT and FROM The simplest SQL query is a query with SELECT and FROM. These are the keywords you will see again and again in SQL. Usually, when constructing a more complex query, it is easier to identify what goes into these two clauses first.\n Create a query that shows all columns from the table you found in Exercise 1, save the dataframe in a variable \u0026ldquo;pitch\u0026rdquo;\n You script should look something like:\nresult = pd.read_sql_query( \u0027SELECT _______ FROM _______\u0027, con) results Excercise 2 WHERE The WHERE keyword allows us to filter down the table horizontally (fewer rows).\nIt goes after SELECT and FROM.\n Using a SQL query, select all rows in the same table where HR is lesser than 10 and gs is greater than 25.\n  Find out what the columns mean and explain your query in words\n Excercise 3 ORDER BY ORDER BY sort the table you select by one or more columns and goes after WHERE\n Using the same query in exercise 2, edit it so that the table is ordered by the year of the season(nearest to furthermost) and the player ID(alphabetically).\n Excercise 4 Joins Joins are used when you wish to create a new table through two different tables. Keep in mind that you have to identify the relationship between two tables before you can correctly join them.\nJOIN goes between FROM and WHERE.\n Identify the shared columns (keys) and join the table in exercise 2 with the salaries table, then filter the data so that it shows only pitchers in the year 1986.\n You should get a dataframe with 306 rows.\nExercise 5 Group by Group by is a keyword we use to lower the level of granularity of a table. Meaning we are combining rows into one by the given column(s).\nCreate a query that captures the number of pitchers the Washington Nationals used in each year, then sort the table by year\nYou should get a dataframe with 23 rows.\nFor the overachievers Excercise 6 Research the order of operations for SQL and put the following keywords in that order.\n SELECT FROM JOIN WHERE HAVING ORDER BY GROUP BY LIMIT  After you have completed this skill builder with your team (or on your own) then compare your work to our script    See the script.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/relational_data\/"},{value:"Week 8-9: Project 4 - Homes",label:"<p></p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p4\/"},{value:"Course Materials",label:"<p>We will be relying on a few resources for this course. You will find the pertinant readings attached to each of the projects. Those readings will be culled from;\n Python for Data Science: A port of R for Data Science using the Python packages pandas and Altair. pandas User Guide Altair User Guide scikit-learn learn User Guide scikit-learn tutorials Python Data Science Handbook A Whirlwind Tour of Python SQL  Wes McKinney\u0026rsquo;s pandas code for his book Python for Data Analysis is a useful reference as well: https:\/\/github.com\/wesm\/pydata-book\n</p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/"},{value:"Machine Learning",label:"<p>Intro to Titanic Machine Learning Skill Builder Link to data\nFor this skill builder, we\u0026rsquo;ll be putting our machine learning hats on. We\u0026rsquo;ll be creating a model that predicts whether a passenger survived. With machine learning, there is a lot of jargon! It can be quite overwhelming at times. This skill builder attempts to keep things basic and simple. With that being said, there are some terms that are important to understand. Let\u0026rsquo;s look at the first few rows of our dataset before proceeding with the definitions.\nThe titanic dataset will be used for examples of each definition.\n   survived pclass sex age siblings_spouses_aboard parents_children_aboard fare     0 3 1 22 1 0 7.25   1 1 0 38 1 0 71.2833   1 3 0 26 0 0 7.925   1 1 0 35 1 0 53.1   0 3 1 35 0 0 8.05    Important Terms:  features: measurable property of the object you\u0026rsquo;re trying to predict. We use this information to predict our target of interest.  Example: pclass, sex, age, siblings_spouses_aboard , parents_children_aboard, fare columns are all examples of different features. Synonyms: attributes, explanatory variables, independent variables, variables, X\u0026rsquo;s, covariates   target: the feature that you are wanting to gain more insight into. The thing you are trying to predict.  Example: in the titanic dataset our target is survived Synonyms: label, dependent variable, y   train set: Usually 70% of the rows from the original dataset are randomly sampled to create this training data. It\u0026rsquo;s used by the algorithm, to determine, or learn, the optimal combinations of variables that will generate a good predictive model  Example: Random sample of 70% of the original titanic dataset rows Synonyms: training data, train data, X_train, y_train   test set: Usually the remaining 30% of the rows in the original dataset are used to create this dataset. The testing data is a set of rows used only to assess the performance (i.e. generalization) of a model. To do this, the final model is used to predict classifications of examples in the test set. Those predictions are compared to the examples\u0026rsquo; true classifications to assess the model\u0026rsquo;s accuracy.  Example: Random sample of 30% of the original titanic dataset rows Synonyms: testing data, test data, X_test, y_test   evaluation metrics: A statistic that tells you how well your predictions align with the actual values. Other words, tells you how good your model is.  Example: Accuracy, Precision, Recall, MSE, MAE, Rsquared Synonyms: performance metric    Again, this is a very light and oversimplified treatment of machine learning. The purpose of this project is to help you understand the main concepts of ml and walk you through the process of building a machine learning model. A simplified work flow of a machine learning project is shown below. Spend some time getting familiar with this flow \u0026amp;mdash as you are about to code it\u0026hellip; Exciting!\nNote in order to do this skill builder you will need to have scikit-learn installed on your machine. Run the following command in your terminal if you haven\u0026rsquo;t already.\npip install scikit-learn\nData Link to csv file\nExercise 0 (Imports and Loading in data) # Loading in packages import pandas as pd import numpy as np import altair as alt from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Loading in data data = pd.read_csv(___)  Exercise 1 Create a chart exploring the relationship between age and survived in the titanic dataset. A strip plot, density plot, or boxplot might be useful here. Below is an example of a density plot. Feel free to replicate this chart or create your own.\nThe purpose of making this chart is to explore the relationships between a feature and the target. We want to see if the feature contains predictive information about the target. This is a large part of machine learning called Exploratory Data Analysis that should never be skipped! Spend time getting to know your features and how they interact with other features and the target.\n Exercise 2 Build a random forest model that is able to predict whether a passenger survived. This exercise is the bulk of the skill builder and contains several steps.\nStep 0: Split the data into X and y variables The X variable will contain all your features\n# Removes the target and keeps all features X = data.drop(___, axis=1) The y variable will hold the target\n# Selects the target column y = data[\u0026#39;___\u0026#39;] Step 1: Split data into train and test sets The train_test_split function is useful for this task. Review the train_test_split function documentation\n# Splitting X and y variables into train and test sets using stratified sampling X_train, X_test, y_train, y_test = train_test_split(___, ___, test_size=0.3, random_state=24, stratify=y) Step 2: Train the model Explore the RandomForestClassifier documentation for the RandomForestClassifier. It\u0026rsquo;s not necessary to understand the inner workings of the Random Forest algorithm for this class - just learn the syntax of fitting the model.\n# Creating random forest object rf = RandomForestClassifier(random_state=24) # Fit with the training data rf.fit(___, ___) Step 3: Use test set to make predictions # Using the features in the test set to make predictions y_pred = rf.predict(___) Step 4: Compare test set predictions to actual values. Calculate the accuracy. # Comparing predictions to actual values accuracy_score(___, ___)  Exercise 3 What is the most important feature in making predictions? Why do you think this is?\nCreate a table that shows the feature importances in descending order. The random forest classifier has a feature importances attribute. It can be accessed by rf.feature_importances_. The table should look something like this.\n   feature names importances     fare 0.288051   sex 0.281853   age 0.266491   pclass 0.0814224   siblings_spouses_aboard 0.0475633   parents_children_aboard 0.034619    After you have completed this skill builder with your team (or on your own) then compare your work to our script    See the script.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/ml_sklearn\/"},{value:"Project 4: Can you predict that?",label:"<p>Background The clean air act of 1970 was the beginning of the end for the use of asbestos in home building. By 1976, the U.S. Environmental Protection Agency (EPA) was given authority to restrict the use of asbestos in paint. Homes built during and before this period are known to have materials with asbestos YOu can read more about this ban.\nThe state of Colorado has a large portion of their residential dwelling data that is missing the year built and they would like you to build a predictive model that can classify if a house is built pre 1980.\nColorado gave you home sales data for the city of Denver from 2013 on which to train your model. They said all the column names should be descriptive enough for your modeling and that they would like you to use the latest machine learning methods.\nData Download: dwellings_denver.csv, dwellings_ml.csv, dwellings_neighborhoods_ml.csv\nInformation: Data description\nReadings  Machine Learning Introduction A visual introduction to machine learning How to choose a good evaluation metric for your Machine learning model  Optional References  Decision Tree Classification in Python Boosted algorithms in scikit-learn scikit-plot package  Grand Questions  Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm. Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried. Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features. Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.  Deliverables Use this template to submit your Client Report. The template has three sections (for additional details please see the instructional template):\n A short summary that highlights key that describes the results describing insights from metrics of the project and the tools you used (Think “elevator pitch”). Answers to the grand questions. Each answer should include a written description of your results, code snippets, charts, and tables.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/project-4\/"},{value:"Project 5: The war with Star Wars",label:"<p>Background Survey data is notoriously difficult to munge. Even when the data is recorded cleanly the options for ‘write in questions’, ‘choose from multiple answers’, ‘pick all that are right’, and ‘multiple choice questions’ makes storing the data in a tidy format difficult.\nIn 2014, FiveThirtyEight surveyed over 1000 people to write the article titled, America’s Favorite ‘Star Wars’ Movies (And Least Favorite Characters). They have provided the data on GitHub.\nFor this project, your client would like to use the Star Wars survey data to figure out if they can predict an interviewing job candidate’s current income based on a few responses about Star Wars movies.\nData Download: StarWars.csv\nInformation: Article\nReadings  Python for Data Science: Tidy Data Python for Data Science: Graphics for Communication Python for Data Science: Strings  Questions and Tasks  Shorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names. Clean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made. Filter the dataset to respondents that have seen at least one film. Create a new column that converts the age ranges to a single number. Drop the age range categorical column. Create a new column that converts the education groupings to a single number. Drop the school categorical column Create a new column that converts the income ranges to a single number. Drop the income range categorical column. Create your target (also known as “y” or “label”) column based on the new income range column. One-hot encode all remaining categorical columns.   Validate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article. Build a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.  Deliverables Use this template to submit your Client Report. The template has three sections (for additional details please see the instructional template):\n A short summary that highlights key that describes the results describing insights from metrics of the project and the tools you used (Think “elevator pitch”). Answers to the grand questions. Each answer should include a written description of your results, code snippets, charts, and tables.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/project-5\/"},{value:"SQL for Data Science",label:"<p>There are many flavors of SQL but most flavors have the same base commands. SQL queries are typed in the following pattern;\nSELECT -- \u0026lt;columns\u0026gt; and \u0026lt;column calculations\u0026gt; FROM -- \u0026lt;table name\u0026gt;  JOIN -- \u0026lt;table name\u0026gt;  ON -- \u0026lt;columns to join\u0026gt; WHERE -- \u0026lt;filter condition on rows\u0026gt; GROUP BY -- \u0026lt;subsets for column calculations\u0026gt; HAVING -- \u0026lt;filter conditions on groups\u0026gt; ORDER BY -- \u0026lt;how the output is returned in sequence\u0026gt; LIMIT -- \u0026lt;number of rows to return\u0026gt; Introductory SQL links  SQL Guide SELECT and FROM clauses WHERE and comparison operators ORDER BY Joins Aggregations GROUP BY  import pandas as pd import altair as alt import numpy as np import sqlite3 # %% # careful to list your path to the file. sqlite_file = \u0026#39;lahmansbaseballdb.sqlite\u0026#39; con = sqlite3.connect(sqlite_file) results = pd.read_sql_query( \u0026#39;SELECT * FROM allstarfull LIMIT 5\u0026#39;, con) results You can see the list of tables available in the database;\ntable = pd.read_sql_query( \u0026#34;SELECT * FROM sqlite_master WHERE type=\u0026#39;table\u0026#39;\u0026#34;, con) print(table.filter([\u0026#39;name\u0026#39;])) print(\u0026#39;\\n\\n\u0026#39;) # 8 is collegeplaying print(table.sql[8]) </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/sql-for-data-science\/"},{value:"Week 6-7: Project 3 - Baseball",label:"<p> We will use a baseball relational database to explore SQL in Python for data science applications. Finding relationships in baseball\n Completed Readings: SQL for Data Science Readings (read all links) and Why SQL is beating NoSQL, and what this means for the future of data\nUse the data.world baseball url for the Data Connection. You can read the Connection Instructions for data.world here\nGrand Questions   Write an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID\/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\n  This three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\n Write an SQL query that provides playerID, yearID, and batting average for players with at least one at bat. Sort the table from highest batting average to lowest, and show the top 5 results in your report. Use the same query as above, but only include players with more than 10 “at bats” that year. Print the top 5 results. Now calculate the batting average for players over their entire careers (all years combined). Only include players with more than 100 at bats, and print the top 5 results.    Pick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc.). Write an SQL query to get the data you need. Use Python if additional data wrangling is needed, then make a graph in Altair to visualize the comparison. Provide the visualization and the compiled Vega script that would build the visualization.\n   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p3\/"},{value:"Munging data",label:"<p>Intro to cleaning movies data Link to the data\nThis skill builder focuses on munging (formatting) data into a machine learning ready dataset. We will be using an IMDB Ratings dataset. It contains columns that are categorical. Sklearn cannot handle columns that are strings, so we need to convert these into a numerical representation. We accomplish this by either one hot encoding, label encoding, or taking just one value of the range provided. There are many other ways to represent these columns as numbers, but they are beyond the scope of this course.\nOnce you\u0026rsquo;ve converted all columns to numeric, in an intelligent way, you will be asked to recreate a graph using altair. Here is the head of the data you will be working with. Enjoy!\n   star_rating content_rating genre duration box_office_rev major_hit     9.3 R Crime 142 €1924521976 - €1925521976 no   9.2 R Crime 175 €177034987 - €178034987 no   9.1 R Crime 200 €2617541398 - €2618541398 no   9 PG-13 Action 152 €996115723 - €997115723 no   8.9 R Crime 154 €1172054364 - €1173054364 no    Data Link to csv file: ...\n Exercise 0  Grab the high range value for each movie and put it into a new column called high_range_rev.  Make sure the data type of this new column is numeric!!   Remove the box_office_rev column from the dataset.  The .str.split() and .astype() methods might be of use! Also, to get the euro sign just copy it from here, €, and put it in your code.\nThe first 5 rows of the resulting dataframe should look like this\n   star_rating content_rating genre duration major_hit high_range_rev     9.3 R Crime 142 no 2345444803   9.2 R Crime 175 no 2182412593   9.1 R Crime 200 no 1604872807   9 PG-13 Action 152 no 284317976   8.9 R Crime 154 yes 1791932201     Exercise 1 Convert the major_hit column to 1\/0\u0026rsquo;s. yes -\u0026gt; 1 and no -\u0026gt; 0. Again, there are several ways to accomplish this. Using our old friend np.where is probably the easiest though.\nThe first 5 rows of the resulting dataframe should like this\n   star_rating content_rating genre duration major_hit high_range_rev     9.3 R Crime 142 0 1925521976   9.2 R Crime 175 0 178034987   9.1 R Crime 200 0 2618541398   9 PG-13 Action 152 0 997115723   8.9 R Crime 154 0 1173054364     Exercise 2 Convert the content_rating column using label encoding. We\u0026rsquo;re using label encoding in this case because the movie ratings already have a natural ordering to them. We will replace each rating with a number in it\u0026rsquo;s natural ascending order.\nTo be more specific, here is how we will do it.\n G: 0 PG: 1 PG-13: 2 R: 3  A dictionary and the .map() method could be useful for this exercise. There are other ways of tackling this problem though. Be creative!\nThe first 5 rows of the resulting dataframe should look like\n   star_rating content_rating genre duration major_hit high_range_rev     9.3 3 Crime 142 0 1925521976   9.2 3 Crime 175 0 178034987   9.1 3 Crime 200 0 2618541398   9 2 Action 152 0 997115723   8.9 3 Crime 154 0 1173054364     Exercise 3 The last column that we need to take care of is genre. We will use one hot encoding for this. Make sure to ONLY one hot encode the genre column!\nA useful function for one hot encoding is pd.get_dummies(). I recommend checking out the documentation.\nThe resulting dataframe should look like the following example; don\u0026rsquo;t worry if your high_range_rev column turned into scientific notation—Pandas does this sometimes.\n    star_rating content_rating duration major_hit high_range_rev genre_Action genre_Adventure genre_Animation genre_Biography genre_Comedy genre_Crime genre_Drama genre_Family genre_Fantasy genre_Horror genre_Mystery genre_Sci-Fi genre_Thriller genre_Western     0 9.3 3 142 0 1.92552e\u002b09 0 0 0 0 0 1 0 0 0 0 0 0 0 0   1 9.2 3 175 0 1.78035e\u002b08 0 0 0 0 0 1 0 0 0 0 0 0 0 0   2 9.1 3 200 0 2.61854e\u002b09 0 0 0 0 0 1 0 0 0 0 0 0 0 0   3 9 2 152 0 9.97116e\u002b08 1 0 0 0 0 0 0 0 0 0 0 0 0 0   4 8.9 3 154 0 1.17305e\u002b09 0 0 0 0 0 1 0 0 0 0 0 0 0 0     Exercise 4 Recreate this graph as best you can. You\u0026rsquo;ll need to use the original data that specifies the actual rating.\nAfter you have completed this skill builder with your team (or on your own) then compare your work to our script    See the script.   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/munging\/"},{value:"Project 6: Git your resume online",label:"<p>Background GitHub is an online platform where data scientists and developers can communicate and share work. As students, you will want to curate your creative work on GitHub using a program called Git. GitHub is the place to share your original work, not your homework assignments.\nMany people store their personal websites, blogs, and project websites on GitHub. Our textbook and course are hosted on GitHub, and you can see J. Hathaway\u0026rsquo;s or Ryan Hafen\u0026rsquo;s personal Data Science websites that are hosted on GitHub as well. For this project, you will be making a public resume that will be hosted on GitHub.\nDuring this project you will learn the process of Git and the tools of GitHub. We will use Git to have others in our class to edit your resume. Take the process seriously (pick a suitable username and write a good resume), and you will have the beginning of your social presence in the DS\/CS space.\nData Repository: Markdown Resume (mdresume) Repository\nInformation: BYUI Data Science Resumes\nReadings  New to Git and GitHub? This Essential Beginners Guide is for you Git vs. GitHub: What is the difference between them? Using Version Control in VS Code Git in Visual Studio Code video  Questions and Tasks  Join GitHub. Pick a username you would be ok sharing with a potential employer. Join the BYUI Data Science Resumes GitHub organization and use the template repository to make a resume repository under your own GitHub account. A good name might be “Lastname-Resume” Clone your repository to your computer and build a first draft of your resume. Push your results to GitHub and have another student fork your repository to make edits. Accept the proposed changes from the student review and finish your final version. After your resume is complete, make sure your it is forked into BYU-I Data Science Resumes  Deliverables:  Complete the questions Submit a URL link to your repo within the byuids-resumes group.  </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/project-6\/"},{value:"VS Code for Data Science",label:"<p> What if my interactive Python window in VS Code is not using the same version of Python as my terminal?    You can set your Python version in VS Code by opening a .py script and then clicking on the Python text in the bottom left corner as shown below.\nOnce you click, VS Code will open the command pallete where you can select your installation of Python that you would like to use with this workspace.\nThis setting will not fix what version your interactive Python window is using. You can get there by opening settings by using the ⌘, shortcut.\nYou can then search your settings for jupyter and you should see a section that has Jupyter Command Line Arguments. Click on the Edit in settings.json.\nHere you can set the jupyter path to Python to match the one you picked for your Terminal. An example for a Mac computer is shown below.\n\t\u0026quot;python.pythonPath\u0026quot;: \u0026quot;\/usr\/local\/opt\/python\/bin\/python3\u0026quot;,    What if I am not able to read in files from the GitHub links using read_csv()?    Most likely your Python SSl certificates are not installed. Follow the answer in this post   How do I use VS Code to collaborate?    Microsft\u0026rsquo;s Live Share extension documentation says, \u0026lsquo;Live Share enables you to quickly collaborate with a friend, classmate, or professor on the same code without the need to sync code or to configure the same development tools, settings, or environment.\u0027 You can follow their guide or use our course created video.\n     </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/vs-code\/"},{value:"Week 4-5: Project 2 - Flights",label:"<p>JSON files are the format of choice for sharing information and data between apps on the internet. When you hear someone explain that you can use an API to get the data, there is usually a JSON file involved. The history of JSON is worth reading. We will have another project analyzing data from JSON files that are missing values. Are we missing JSON on our flight?\n Completed Readings: P4DS: Chapter 5 Data tranformation, P4DS: Section 7.4 Missing Values, Python Data Science Handbook: Missing Data, How to Handle Missing Data, and Wikipedia Missing Data\n The flights JSON File\nand the Data Description\n ### Grand Questions  1. __Which airport has the worst delays? How did you choose to define \u0022worst\u0022? As part of your answer include a table that lists the total number of flights, total number of delayed flights, proportion of delayed flights, and average delay time in hours, for each airport.__   2. __What is the worst month to fly if you want to avoid delays? Include one chart to help support your answer, with the x-axis ordered by month. You also need to explain and justify how you chose to handle the missing `Month` data.__   3. __According to the BTS website the Weather category only accounts for severe weather delays. Other “mild” weather delays are included as part of the NAS category and the Late-Arriving Aircraft category. Calculate the total number of flights delayed by weather (either severe or mild) using these two rules:__   1. __30% of all delayed flights in the Late-Arriving category are due to weather.__  2. __From April to August, 40% of delayed flights in the NAS category are due to weather. The rest of the months, the proportion rises to 65%.__   4. __Create a barplot showing the proportion of all flights that are delayed by weather at each airport. What do you learn from this graph (Careful to handle the missing `Late Aircraft` data correctly)?__   5. __Fix all of the varied `NA` types in the data and save the file back out in the same format that was provided. Provide one example from the file with the new `NA` values shown.__ --------------------------------------------------- </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p2\/"},{value:"Altair for Charts",label:"<p>Altair Visualization We will be using Altair in our course. It is a declarative visualization package in Python that is based on Vega-Lite which leverages the grammar of graphics.\n User Guide Data Visualization Curriculum or the Quarto version https:\/\/jjallaire.github.io\/visualization-curriculum\/) P4DS Data Visualization Chapter  Rendering Altair Charts in Quarto We use Quarto to render Altair images automagically into our HTML reports. The process should simply work. However, read in the following section if you need to export one of your images as a .png or another image format.\nSaving Altair Charts Just installing altair and altair_saver will not allow you to leverage the .save() method to save your chart. The javascript visualization you see in your interactive python window needs additional external applications to allow .save(\u0027chart.png\u0027) to work.\nWe will go through a few ways for us to save our Altair plots.\n  1. Saving altair plots programmatically Let\u0026rsquo;s say we want to save the above plot as a PNG file. Assuming we have already installed the altair library, we need to install the altair_saver.\n1.1 Installing the altair_saver Within your interactive python window execute the following command.\nimport sys !{sys.executable} -m pip install altair_saver 1.2 Additional tool for saving plots We suggest NodeJS path. However, you are more than welcome to study Selenium for further understanding. The Github repository for altair_saver, the developers exclusively told us to install additional tools.\nNodeJS Installation\n Install the NodeJS for your platform Run the following in your Terminal (Mac) or PowerShell (Windows) to install all the packages we need from NodeJS.  npm install -g vega-lite vega-cli canvas M1 Mac Altair Solution  Install selenium using the chromedriver package form this link: https:\/\/chromedriver.chromium.org. Unzip the file and move the file to your chrome path \/usr\/local\/bin\/chromedriver  See the selenium_fix.py script for an example.\nNote: This process will run a local server on your computer that opens the chart as an PNG file in chrome and downloads the file to the folder in which that VSCode file is located on your computer.\n1.3 Saving a plot using altair_saver It might require you to restart VScode and import everything again for this to work. Please note that the plot will be saved in the same folder of the script.\nchart = alt.chart(\u0026lt;data\u0026gt;).\u0026lt;chart_methods\u0026gt; chart.save(\u0026#39;name_of_chart.png\u0026#39;) 2. Save as PNG method The method only requires us to have Altair library. Whenever we output a plot, we will see a button with three dots at the top right corner of the plot.\nClicking Save as PNG will bring us to a window to save our plot.\n3. Screenshot method If all thing fails and we need to save a plot, the snip \u0026amp; sketch (Windows) or taking a screenshot (MacOS) will be our last resort.\n</p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/altair\/"},{value:"GitHub and git",label:"<p>Complete the Hello World GitHub Guide\n</p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/git_github\/"},{value:"Quarto for Data Science",label:"<p>Quarto Quarto is an open-source scientific and technical publishing system built on Pandoc. You can create dynamic content with Python, R, Julia, and Observable.\nWe use this perfect union of Jupyter Notebooks and RMarkdown for reporting on our projects. It leverages Markdown and Python code chunks to create dynamic HTML content.\nMarkdown Markdown is a plain text formatting syntax aimed at making writing more accessible. The philosophy behind Markdown is that plain text documents should be readable without tags making a mess, but there should still be ways to add text modifiers like lists, bold, italics, etc. It is an alternative to WYSIWYG (what you see is what you get) editors, which use rich text that later gets converted to proper HTML.1\nQuarto Basics You will need to install the Quarto CLI and then go through the VS Code directions on using Quarto with Python.\n Install Quarto CLI Setup your VS Code Really read the VS Code setup entirely  Class template We have built a template to provide an example of you will submit your project reports (for additional details please see the instructional template). As you use the template, the following items may help you understand how to write your report.\n The template is a guide. Don\u0026rsquo;t feel responsible for including every item beyond sections for each question. Your appendix should have properly highlighted Python code that doesn\u0026rsquo;t run off the page (other than file paths). You can see examples of the html output here and here  Markdown Examples You can read the complete syntax guide at the daringfireball.net website. The code chunk below highlights the standard syntax2\n*This text will be italic* _This will also be italic_ **This text will be bold** __This will also be bold__ _You **can** combine them_ You can make bulleted lists. * Item 1 * Item 2 * Item 2a * Item 2b Or numbered lists. 1. Item 1 1. Item 2 1. Item 3 1. Item 3a 1. Item 3b Place an image in the document. ![GitHub Logo](\/images\/logo.png) or a link in a document [GitHub](http:\/\/github.com) You can even blockquote Kanye West said: \u0026gt; We\u0026#39;re living the future so \u0026gt; the present is our past.  Finally, you can create tables. Check out `print(df.to_markdown())` to get tables from pandas. First Header | Second Header ------------ | ------------- Content from cell 1 | Content from cell 2 Content in the first column | Content in the second column Every once in a while, you may want strikethrough. ~~this~~ Getting tables out of Pandas You can create tables using Markdown in your reports. You can use the .to_markdown() method on your DataFrame object. You would use print(df.to_markdown(index=False)) to get tables from pandas. They would print out in your interactive window as;\nname | gender ----- | ------ J. | Male Katie | Female You would then copy the output from your interactive window and paste it into your .md report.\n  https:\/\/www.ultraedit.com\/company\/blog\/community\/what-is-markdown-why-use-it.html \u0026#x21a9;\u0026#xfe0e;\n https:\/\/guides.github.com\/features\/mastering-markdown\/ \u0026#x21a9;\u0026#xfe0e;\n   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/quarto-for-data-science\/"},{value:"Week 2-3: Project 1 - Names",label:"<p>We are going to start learning the pandas package while we explore the names data for our project. What is in a name?\n Completed Readings: Python for Data Science (P4DS): Data Visualization, P4DS: Graphics for Communication, P4DS: Markdown, P4DS: 5.2 Filter rows with .query()\n https:\/\/github.com\/byuidatascience\/data4names\/raw\/master\/data-raw\/names_year\/names_year.csv\n ### Grand Questions  1. __How does your name at your birth year compare to its use historically?__  1. __If you talked to someone named Brittany on the phone, what is your guess of their age?__  1. __Mary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names.__  1. __Think of a unique name from a famous movie. Plot that name and see increases line up with the movie release.__ ------------------------------ </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/p1\/"},{value:"Git and GitHub for DS",label:"<p>Git what? Git is a distributed version control tool that can manage a development project\u0026rsquo;s source code history, while GitHub is a cloud based platform built around the Git tool. Git is a tool a developer installs locally on their computer, while GitHub is an online service that stores code pushed to it from computers running the Git tool. The key difference between Git and GitHub is that Git is an open-source tool developers install locally to manage source code, while GitHub is an online service to which developers who use Git can connect and upload or download resources.1\nGit? The Git tool is popular with developers because is stays true to its purpose of versioning source code, managing commit histories and making it possible to share code between developers without deviating into peripheral fields. There is no feature bloat with Git. It does what it does, it does nothing else, and it makes no apologies for that fact.1\nGithub? We’ve established that Git is a version control system, similar but better than the many alternatives available. So, what makes GitHub so special? Git is a command-line tool, but the center around which all things involving Git revolve is the hub—GitHub.com—where developers store their projects and network with like minded people.2\nSteps related to Git and Github for our final project.   Make sure you have git on your computer.\nA. Note that Mac users have a few extra concerns.3 B. Mac fix with paths ls \/usr\/local C. Download Xcode and update 10 gig download.\nD. VSCode path selection settings Git: path    Create a GitHub account and use an appropriate username    Connect to our BYU-I organizations.\nA. BYU-I DS Resumes need teacher to admit you B. BYU-I Data Science Society need teacher to admit you    Creat your own resume repo from our template (some directions)[https:\/\/github.blog\/2019-06-06-generate-new-repositories-with-repository-templates\/]    Publish your repo on GitHub pages.\nA. Go to settings for your repo.\nB. Scroll down to the GitHub Pages section.\nC. Under source select the box which says None and pick master.\nD. Now select the \/docs folder and click save.    Check your published site settings and copy your site URL.    Update your repository landing page to include your pages URL.    Edit the readme.md in the base repo to not show the resume directions if your repo is public.    Fork your repo back into the BYU-I DS Resumes    Merge a pull request with any changes in your personal repository (see pull and merge on GitHub Guide).     https:\/\/www.theserverside.com\/video\/Git-vs-GitHub-What-is-the-difference-between-them#:~:text=The%20key%20difference%20between%20Git,and%20upload%20or%20download%20resources. \u0026#x21a9;\u0026#xfe0e;\n https:\/\/www.howtogeek.com\/180167\/htg-explains-what-is-github-and-what-do-geeks-use-it-for\/ \u0026#x21a9;\u0026#xfe0e;\n https:\/\/stackoverflow.com\/questions\/29971624\/visual-studio-code-cannot-detect-installed-git \u0026#x21a9;\u0026#xfe0e;\n   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/course-materials\/git_github_ds\/"},{value:"Week 1: Introduction",label:"<p>  Introduction Project Syllabus   </p><p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/introduction\/"},{value:"DS250",label:"<p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/"},{value:"Frequently Asked Questions",label:"<p> What do you mean by data science programming?    Most likely, you have had 1-2 courses of programming before you have taken DS 250. Unlike traditional computer science courses, DS 250 uses Python in an interactive mode instead of building programs. The data provider usually has some big questions that need answering; However, there are hundreds of little issues and responses along the way. We use programming to facilitate this investigation.\nThere are similarities with User Experience Designers. In our case, we don\u0026rsquo;t get to ask users about their experience. We use programming to ask data about its background, and each data set has its own history. We want our analysis to mold to that experience. You can think of data science programming like a first date with your data. You can\u0026rsquo;t write one long program nieve of the issues and nuances each living data set provides.\n   How does DS 250 compare to DS 350?    The two courses have similarities. You could think of DS 250 as an introduction to data wrangling and visualization. Both classes use real-world data and are built around data science projects. There are some critical differences between the two courses.\n In this course, we use Python, and DS 350 uses R. We are introducing the principles of data science programming in DS 250. The course is only 2-credits. DS 250 is intended to introduce visualization, wrangling, and modeling.     How does DS 250 prepare me for DS 350 and CSE 450?    You will be comfortable with interactive programming and have an introduction to the principles of data formats for data science applications. You will be introduced to principles related to machine learning, data wrangling, and data visualization.   What programming languages do we use in this course?    The course is done using Python. We focus on the pandas and Altair packages.   What are the prerequisites for this course?    Using the new courses at BYU-I, the prerequisite is CSE 110. However, if you have experience programming from other classes, you most likely are prepared for this course.   Why Python instead of R?    The computer science and software engineering programs at BYU-I use Python as their foundational courses. The standard student will have some experience with Python before DS 250. Python is an essential programming language for data scientists, and we already have DS350, which is taught in R.   What is pandas?    pandas is the foundational data science package in Python. If you are using tabular data you will be in pandas.   Why are we using Altair instead of Seaborn or Matplotlib?    Matplotlib was the first visualization package to gain a following in Python. Seaborn is built on top of Matplotlib. Many data scientists use both in their work—neither leverage the grammar of graphics as developed by Leland Wilkinson. Altair is built on Vega-Lite, which uses the Vega visualization grammar. It is declarative and actively developed. We expect that it will become the predominant visualization package in Python (https:\/\/youtu.be\/FytuB8nFHPQ and https:\/\/youtu.be\/vTingdk_pVM).   </p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/faq\/"},{value:"Projects",label:"<p>We will be relying on a few resources for this course. You will find the pertinant readings attached to each of the projects. Those readings will be culled from;\n Python for Data Science: A port of R for Data Science using the Python packages pandas and Altair. pandas User Guide Altair User Guide scikit-learn learn User Guide scikit-learn tutorials Python Data Science Handbook A Whirlwind Tour of Python SQL  Wes McKinney\u0026rsquo;s pandas code for his book Python for Data Analysis is a useful reference as well: https:\/\/github.com\/wesm\/pydata-book\n</p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/projects\/"},{value:"Skill Builders",label:"<p>These short activites are provided for you to gain some additional skills to help with the class projects.\n</p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/skill_builders\/"},{value:"Slides",label:"<p>Use the navigation pane on the left to review the class slides.\n</p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/slides\/"},{value:"Categories",label:"<p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/categories\/"},{value:"Office Hours",label:"<p>Schedule a time with your teacher. You can find the office hours of the BYU-I data science faculty at our website or by typing https:\/\/byuidatascience.github.io\/visit\/.\n</p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/contact\/"},{value:"Tags",label:"<p></p>",url:"https:\/\/byuistats.github.io\/DS250-Comeau\/tags\/"},];$("#search").autocomplete({source:projects}).data("ui-autocomplete")._renderItem=function(ul,item){return $("<li>").append("<a href="+item.url+" + \" &quot;\" +  >"+item.value+"</a>"+item.label).appendTo(ul);};});</script></div></div></div></div></header><section class=section><div class=container><div class="row justify-content-center"><div class="col-12 text-center"><h2 class=section-title></h2></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://byuistats.github.io/DS250-Comeau/course-materials/ class="px-4 py-5 bg-white shadow text-center d-block match-height"><i class="ti-blackboard icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Course Materials</h3><p class=mb-0>Additional Readings and Guidance</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://byuistats.github.io/DS250-Comeau/projects/ class="px-4 py-5 bg-white shadow text-center d-block match-height"><i class="ti-bar-chart icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Projects</h3><p class=mb-0>Project details (the work you will do)</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://byuistats.github.io/DS250-Comeau/skill_builders/ class="px-4 py-5 bg-white shadow text-center d-block match-height"><i class="ti-ruler-pencil icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Skill Builders</h3><p class=mb-0>Build skills for the projects.</p></a></div><div class="col-lg-4 col-sm-6 mb-4"><a href=https://byuistats.github.io/DS250-Comeau/slides/ class="px-4 py-5 bg-white shadow text-center d-block match-height"><i class="ti-layout-slider-alt icon text-primary d-block mb-4"></i><h3 class="mb-3 mt-0">Slides</h3><p class=mb-0>Class material for every day.</p></a></div></div></div></section><footer class="section pb-4"><div class=container><div class="row align-items-center"><div class="col-md-8 text-md-left text-center"><p class="mb-md-0 mb-4">J. Hathaway and BYU-I ©</p></div><div class="col-md-4 text-md-right text-center"><ul class=list-inline><li class=list-inline-item><a class="text-color d-inline-block p-2" href=https://github.com/byuidatascience><i class=ti-github></i></a></li><li class=list-inline-item><a class="text-color d-inline-block p-2" href=https://www.linkedin.com/groups/13537407/><i class=ti-linkedin></i></a></li></ul></div></div></div></footer><script src=https://byuistats.github.io/DS250-Comeau/js/script.min.js></script></body></html>